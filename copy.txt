# path: debug\dqn_1_env_cartpole.py
if __name__ == "__main__":
    import os
    import sys
    import inspect

    currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
    parentdir = os.path.dirname(currentdir)
    sys.path.insert(0, parentdir) 

from rl_agents.service import AgentService
from rl_agents.value_agents.double_q_net import  DoubleQNNProxy, SoftDoubleQNNProxy
from rl_agents.policies.epsilon_greedy_proxy import EspilonGreedyPolicy
from rl_agents.policies.value_policy import QValuePolicy
from rl_agents.replay_memory.replay_memory import ReplayMemory, MultiStepReplayMemory
from rl_agents.replay_memory.sampler import PrioritizedReplaySampler, RandomSampler
from rl_agents.value_functions.dqn_function import DQNFunction
from rl_agents.value_agents.dqn import DQNAgent
from rl_agents.trainers.trainer import Trainer

import torch
import numpy as np
import gymnasium as gym


class QNN(AgentService):
    def __init__(self, observation_space : gym.spaces.Space, action_space : gym.spaces.Discrete, hidden_dim :int, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # We suppose observation_space and action_space to be 1D
        self.module_list = torch.nn.ModuleList()
        for i in range(2):
            self.module_list.add_module(f"lin_{i}", torch.nn.Linear(in_features= observation_space.shape[0] if i ==0 else hidden_dim, out_features= hidden_dim))
            self.module_list.add_module(f"act_{i}", torch.nn.ReLU())
        self.head = torch.nn.Linear(in_features=hidden_dim, out_features=action_space.n)
    
    def forward(self, state : torch.Tensor, **kwargs) -> torch.Tensor:
        # state : [batch, nb_obs]
        x = state
        for module in self.module_list:
            x = module(x)
        return self.head(x)

def main():
    # gym.make("LunarLander-v3")
    env = gym.make("CartPole-v1")

    action_space = env.action_space # 0 : short , 1 : long
    observation_space = env.observation_space # open, high, low, close, volume

    nb_env = 1
    memory_size = 1E5
    gamma = 0.99
    multi_step = 3

    replay_memory = MultiStepReplayMemory(
        max_length = memory_size,
        multi_step=multi_step,
        nb_env=nb_env,
        gamma=gamma,
        observation_space= observation_space
    )
    sampler= RandomSampler(
        replay_memory=replay_memory
    )
    # sampler= PrioritizedReplaySampler(replay_memory=replay_memory, batch_size = 64, duration= 100_000),

    q_net  = QNN(observation_space=observation_space, action_space= action_space, hidden_dim= 128)
    q_net = SoftDoubleQNNProxy(
        q_net = q_net,
        tau= 1/50
    )
    q_function = DQNFunction(
        net= q_net,
        gamma= gamma,
        loss_fn= torch.nn.MSELoss()
    )
    policy = EspilonGreedyPolicy(
        q = 1 - 1E-3,
        start_epsilon= 0.9,
        end_epsilon= 0.01,
        action_space= action_space,
        policy= QValuePolicy(q_function= q_function)
    )
    agent = DQNAgent(
        nb_env= nb_env,
        policy= policy,
        train_every= 1,
        q_function= q_function,
        replay_memory=replay_memory,
        sampler=sampler,
        optimizer= torch.optim.Adam(params=q_net.parameters(), lr = 1E-3),
        batch_size=64
    )

    print("optimizer param count:", sum(p.numel() for p in agent.optimizer.param_groups[0]['params'] if p is not None))

    agent.train()
    episodes = 1000
    for i in range(episodes):
        episode_rewards = 0
        episode_losses = []
        episode_steps = 0

        truncated = False
        done = False
        state, infos = env.reset()

        while not truncated and not done:
            action = agent.pick_action(state= state)
            next_state, reward, done, truncated, infos = env.step(action = int(action))
            episode_rewards += reward
            # print(state, action, reward, next_state, done, truncated)
            done = done or truncated
            agent.store(state = state, action = action, reward = reward, next_state = next_state, done = done, truncated=truncated)
            loss = agent.train_agent()

            if loss is not None: episode_losses.append(loss)
            
            episode_steps += 1
            state = next_state

        epsilon = policy.epsilon
        episode_loss = np.array(episode_losses).mean()
        print(f"Episode {i:3d} - Steps : {episode_steps:4d} | Total Rewards : {episode_rewards:7.2f} | Loss : {episode_loss:0.2e} | Epsilon : {epsilon : 0.2f} | Agent Step : {agent.step}")
        # print(episode_losses)

if __name__ == "__main__":
    import sys
    sys.path.append("../")
    main()

# end of file: debug\dqn_1_env_cartpole.py

# path: rl_agents\agent.py
from rl_agents.policies.policy import AbstractPolicy
from rl_agents.service import AgentService

from abc import ABC, abstractmethod
import torch
import numpy as np


class AbstractAgent(AbstractPolicy, AgentService, ABC):
    def __init__(
        self,
        nb_env: int,
        policy: AbstractPolicy
    ):
        AgentService.__init__(self)
        AbstractPolicy.__init__(self)
        self.nb_env = nb_env
        self.policy = policy

        self.episode = 0
        self.step = 0

    def update(self, agent : 'AbstractAgent'):
        self.step += 1

    @abstractmethod
    def train_agent(self):
        assert self.training, "Please set the agent in training mode using .train()"
    
    def pick_action(self, state : np.ndarray):
        state = torch.as_tensor(state, dtype=torch.float32)

        single_env_condition = self.nb_env == 1 and (state.shape[0] != 1 or state.ndim == 1)
        if single_env_condition: state = state.unsqueeze(0)

        pick_action_return = self.policy.pick_action(agent=self, state=state)
        
        if single_env_condition: 
            if isinstance(pick_action_return, torch.Tensor): pick_action_return = pick_action_return.squeeze(0)
            elif isinstance(pick_action_return, tuple): pick_action_return = tuple([elem.squeeze(0) for elem in pick_action_return])
            else: raise ValueError("Pick action must be a Tensor or a tuple of Tensor")

        if isinstance(pick_action_return, torch.Tensor): pick_action_return = pick_action_return.detach().numpy()
        elif isinstance(pick_action_return, tuple): pick_action_return = tuple([elem.detach().numpy() for elem in pick_action_return])
        else: raise ValueError("Pick action must be a Tensor or a tuple of Tensor")

        self.__update__(agent=self)
        return pick_action_return

# end of file: rl_agents\agent.py

# path: rl_agents\policies\epsilon_greedy_proxy.py
from rl_agents.agent import AbstractAgent
from rl_agents.policies.policy import AbstractPolicy
import numpy as np
import torch
from abc import ABC, abstractmethod
from gymnasium.spaces import Space
import math

class BaseEspilonGreedyPolicy(AbstractPolicy, ABC):
    def __init__(self, policy : AbstractPolicy, action_space: Space, epsilon : int):
        super().__init__()
        self.policy = policy
        self.action_space = action_space
        self.epsilon = epsilon

    @abstractmethod
    def epsilon_function(self, agent: AbstractAgent): ...

    def pick_action(self, agent: AbstractAgent, state: torch.Tensor):

        if not agent.training:
            return self.policy.pick_action(agent, state)
        # state : (nb_env, ...)
        rands = torch.rand(agent.nb_env)  # shape : (nb_env,)
        env_random_action = rands < self.epsilon
        env_model_action = ~env_random_action
        actions = torch.zeros(
            size=(agent.nb_env,) + self.action_space.shape
        ).long() # shape (nb_env, action_shape ...)
        if env_model_action.any():
            masked_state = state[
                env_model_action
            ]  # shape (nb_env_selected,  state_shape ...)
            model_actions = self.policy.pick_action(agent = agent, state = masked_state)
            actions[env_model_action] = model_actions.long()

        if env_random_action.any():
            nb_random = env_random_action.sum()
            random_actions = torch.Tensor(
                [self.action_space.sample() for i in range(nb_random)]
            ).long()
            actions[env_random_action] = random_actions

        if agent.nb_env == 1:
            actions = actions[0]
        return actions

    def update(self, agent: AbstractAgent):
        self.epsilon = self.epsilon_function(agent=agent)


class EspilonGreedyPolicy(BaseEspilonGreedyPolicy):
    def __init__(self, policy : AbstractPolicy, q: float, action_space: Space, end_epsilon: float, start_epsilon : float = 1):
        super().__init__(policy= policy, action_space=action_space, epsilon= start_epsilon)
        self.q = q
        self.start_epsilon = start_epsilon
        self.end_epsilon = end_epsilon

    def epsilon_function(self, agent):
        return max(self.end_epsilon, self.end_epsilon + (self.start_epsilon - self.end_epsilon) *self.q**agent.step)


# end of file: rl_agents\policies\epsilon_greedy_proxy.py

# path: rl_agents\replay_memory\replay_memory.py
from rl_agents.replay_memory.sampler import AbstractSampler, RandomSampler
from rl_agents.service import AgentService

from abc import ABC, abstractmethod
from collections import deque
import torch
import numpy as np
from gymnasium.spaces import Space, Box
from functools import partial
import warnings
from dataclasses import dataclass, make_dataclass, asdict, fields

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from rl_agents.agent import AbstractAgent

torch.utils.data.TensorDataset
class AbstractReplayMemory(torch.utils.data.Dataset, AgentService, ABC):
    @abstractmethod
    def store(self, agent: "AbstractAgent", **kwargs): ...

    @abstractmethod
    def reset(self): ...

    _max_length = None
    @property
    def max_length(self) -> int:
        if self._max_length is None: raise AttributeError(f"{self.__class__.__name__}.max_length is not set")
        return self._max_length
    @max_length.setter
    def max_length(self, val): self._max_length = val


@dataclass(kw_only=True, slots=True)
class Experience:
    def __getattr__(self, item):
        # Called only if attribute not found normally
        raise AttributeError(
            f"'The exprience {type(self).__name__}' has no attribute '{item}'. Please add it the replay_memory"
            f"Available attributes: {[field.name for field in fields(self)]}"
        )

@dataclass(kw_only=True, slots=True)
class ExperienceSample:
    indices : torch.Tensor
    def __getattr__(self, item):
        # Called only if attribute not found normally
        raise AttributeError(
            f"'The replay_memory related to {type(self).__name__}' has no attribute '{item}'. Your replay_memory is probably not meant to be use in this context."
            f"Available attributes: {[field.name for field in fields(self)]}"
        )

class BaseReplayMemory(AbstractReplayMemory):
    def __init__(
        self,
        max_length: int,
        fields: list[tuple[str, tuple, torch.dtype]],
        device: torch.DeviceObjType = None,
    ):
        torch.nn.Module.__init__(self)
        AgentService.__init__(self)
        self.max_length = int(max_length)
        self.device = device

        # Unpack fields
        self.fields  = fields
        self._mandatory_fields = set()
        self.names, self.sizes, self.dtypes, self.default_values = [], {}, {}, {}
        for field in fields: self._set_up_field(*field) # Field : (name, shape, dtype, (optional) default_value)
        self._generate_dataclasses()
        self.i = 0


    def _generate_dataclasses(self):
        # Create the dataclasses dynamically
        experience_fields = [(name, torch.Tensor) for name in self.names]

        self.experience_dataclass_generator = make_dataclass(
            f"Experience{self.__class__.__name__}",
            fields=experience_fields,
            bases=(Experience,),
            kw_only=True,
            slots=True,
        )
        self.sample_dataclass_generator = make_dataclass(
            f"ExperienceSample{self.__class__.__name__}",
            fields=experience_fields,
            bases=(ExperienceSample,),
            kw_only=True,
            slots=True,
        )

    def _set_up_field(self, name : str, shape : tuple[int], dtype : None, default_value = None):
        assert name not in self.names, f"Found duplicate fields {name} in {self.names}"
        self.names.append(name)
        self.sizes[name] = (self.max_length,) + shape
        self.dtypes[name] = dtype
        if default_value is not None: self.default_values[name] = default_value
        else: self._mandatory_fields.add(name)
        self._reset_tensor(name=name)

    def add_field(self, name : str, shape : tuple[int], dtype : None, default_value = None):
        self._set_up_field(name=name, shape=shape, dtype=dtype, default_value=default_value)
        self._generate_dataclasses()


    def reset(self, name = None):
        for name in self.names: 
            self._reset_tensor(name = name)
        self.i: int = 0

    def _reset_tensor(self, name : str):
        fill_value = 0
        if name in self.default_values:fill_value=self.default_values[name]

        self.register_buffer(
            name=f"memory_{name}",
            tensor=torch.full(size = self.sizes[name], fill_value=fill_value, dtype = self.dtypes[name]),
            persistent=True,
        )
    def __len__(self):
        return min(self.i, self.max_length)

    @torch.no_grad()
    def store(self, agent: "AbstractAgent", experience = None, **kwargs):
        """Save a experience"""
        
        if experience is not None: kwargs.update(asdict(experience))

        assert self._mandatory_fields.issubset(kwargs.keys()), (
            f"Missing fields. Expected at least {sorted(self._mandatory_fields)}, got {sorted(kwargs.keys())}"
        )
        
        i = int(self.i % self.max_length)

        nb_env = next(iter(kwargs.values())).size(0)
        for name, val in kwargs.items():
            try:
                val_tensor = torch.as_tensor(val, dtype = self.dtypes[name])
            except KeyError:
                warnings.warn(f"{name} does not exist as a field in {self.__class__.__name__}. Current fields : {', '.join(self.names)}")
            else:
                if i+nb_env <= self.max_length: getattr(self, f"memory_{name}")[i:i+nb_env] = val_tensor
                else:
                    n_end = self.max_length - i
                    n_start = nb_env - n_end
                    getattr(self, f"memory_{name}")[i:i+n_end] = val_tensor[:n_end]
                    getattr(self, f"memory_{name}")[0:n_start] = val_tensor[n_end:n_end + n_start]

        
        self.i += nb_env
        # self.sampler.store(agent=agent, replay_memory = self, **kwargs)

    # @torch.no_grad()
    # def sample(self, agent : "AbstractAgent", batch_size: int) -> ExperienceSample:
    #     if self.__len__() < batch_size:
    #         return

    #     batch_sample = self.sampler.sample(agent=agent, batch_size=batch_size, size=self.__len__())
    #     return self[batch_sample]


    def __getitem__(self, loc) -> ExperienceSample:
        index : int | np.ndarray
        name :str
        if isinstance(loc, tuple):
            name, index= loc
            return getattr(self, f"memory_{name}")[index]
        elif isinstance(loc, str):
            name = loc
            return getattr(self, f"memory_{name}")[:self.__len__()]
        else:
            indices = torch.as_tensor(loc)
            return self.sample_dataclass_generator(
                indices=indices,
                **asdict(self.__get_experience_from_indices__(indices=indices))
            )
    def __getitems__(self, indices):
        return self.__getitem__(loc = indices)
    
    def __get_experience_from_values__(self, **kwargs): return self.experience_dataclass_generator(**{name : torch.as_tensor(value, dtype = self.dtypes[name]) for name, value in kwargs.items()})
    def __get_experience_from_indices__(self, indices): return self.experience_dataclass_generator(**{name : getattr(self, f"memory_{name}")[indices] for name in self.names})

    def __setitem__(self, loc, val):
        indices : int | np.ndarray
        name :str
        if isinstance(loc, tuple):
            name, indices= loc
            getattr(self, f"memory_{name}")[indices] = val
        elif isinstance(loc, str):
            name = loc
            getattr(self, f"memory_{name}")[:self.__len__()] = val


class ReplayMemory(BaseReplayMemory):
    def __init__(
        self,
        max_length: int,
        observation_space: Space,
        device: torch.DeviceObjType = None,
    ):
        assert isinstance(
            observation_space, Box
        ), "ReplayMemory only supports gymnasium.spaces.Box as observation_space"
        self.observation_space = observation_space

        super().__init__(
            max_length=max_length,
            fields=[
                ("state",       self.observation_space.shape,   torch.float32),
                ("action",      (),                             torch.long),
                ("next_state",  self.observation_space.shape,   torch.float32),
                ("reward",      (),                             torch.float32),
                ("done",        (),                             torch.bool),
                ("truncated",   (),                             torch.bool),
            ],
            device=device,
        )


class MultiStepReplayMemory(BaseReplayMemory):
    """
    n-step replay pour plusieurs environnements parallèles.
    Les arguments d'entrée de .store doivent avoir shape (nb_env, …).
    """

    def __init__(
        self,
        max_length: int,
        observation_space: Box,
        nb_env: int,
        gamma: float,
        multi_step: int,
        device: torch.DeviceObjType = None,
    ):
        warnings.warn(
            f"When using {self.__class__.__name__}, please provide the multi_step parameter "
            "for the services that support it (e.g.: DQNFunction, DistributionalDQNFunction ...)"
        )
        assert isinstance(observation_space, Box)
        self.multi_step, self.gamma, self.nb_env = multi_step, gamma, nb_env
        self.buffers = [deque(maxlen=multi_step) for _ in range(nb_env)]

        super().__init__(
            max_length=max_length,
            fields=[
                ("state",       observation_space.shape,    torch.float32),
                ("action",      (),                         torch.long),
                ("next_state",  observation_space.shape,    torch.float32),
                ("reward",      (),                         torch.float32),
                ("done",        (),                         torch.bool),
                ("truncated",   (),                         torch.bool),
            ],
            device=device,
        )

        # pré-calc γ^k pour l’agrégation vectorielle
        self.register_buffer("_gammas", 
            tensor = gamma ** torch.arange(multi_step, device=device, dtype=torch.float32)
        )

    def _aggregate(self, buf: deque):
        """
        Convertit le contenu d'un deque en transition n-step.
        -> retourne un dict prêt pour super().store(**transition)
        """
        # Empile récompenses pour un produit scalaire vectoriel
        r = torch.stack([e.reward for e in buf])  # (L,)
        R = torch.dot(self._gammas[: len(r)], r)  # scalaire  γ^k * r_k

        return dict(
            state=buf[0].state[None, ...],
            action=buf[0].action[None, ...],
            next_state=buf[-1].next_state[None, ...],
            reward=R[None, ...],
            done=buf[-1].done[None, ...],
            truncated=buf[-1].truncated[None, ...],
        )


    # ---- API publique ----------------------------------------------
    @torch.no_grad()
    def store(self,  agent: "AbstractAgent", **kwargs):
        """
        `state`, `action`, … : tenseurs dont la 0-ème dim = nb_env.
        """
        
        # Boucle fine sur les envs ; la plupart du temps nb_env <= 16, négligeable.
        for env_id in range(self.nb_env):
            kwargs_env = {key : val[env_id] for key, val in kwargs.items()}
            experience_env = self.__get_experience_from_values__(**kwargs_env)

            buf = self.buffers[env_id]
            buf.append(experience_env) # We use tensor[env_id:env_id+1] to select the one elem corresponding 

            # fenêtre pleine : pousse une transition n-step
            if len(buf) == self.multi_step:
                super().store(agent=agent, **self._aggregate(buf))
                buf.popleft()  # fenêtre glissante

            # fin d'épisode : flush des restes
            if experience_env.done or experience_env.truncated:
                while buf:
                    super().store(agent=agent, **self._aggregate(buf))
                    buf.popleft()




# end of file: rl_agents\replay_memory\replay_memory.py

# path: rl_agents\replay_memory\sampler.py
from rl_agents.value_agents.value_agent import AbstractValueAgent
from rl_agents.service import AgentService
from rl_agents.utils.sumtree import SumTree

from abc import ABC, abstractmethod
import numpy as np
import torch
from dataclasses import dataclass
from functools import partial

from typing import TYPE_CHECKING, Optional, Callable

if TYPE_CHECKING:
    from rl_agents.agent import AbstractAgent
    from rl_agents.replay_memory.replay_memory import AbstractReplayMemory


class AbstractSampler(AgentService, torch.utils.data.Sampler, ABC):
    def update(self, agent: "AbstractAgent"): ...

    def store(self, agent: "AbstractAgent", **kwargs): ...

    def compute_weights_from_indices(self, indices : torch.Tensor):...

    def update_experiences(self, agent : "AbstractAgent", **kwargs): ...

class RandomSampler(AbstractSampler):
    def __init__(self, replay_memory : "AbstractReplayMemory"):
        super().__init__()
        self.replay_memory = replay_memory

    @torch.no_grad()
    def __len__(self) -> int:
        return len(self.replay_memory)

    @torch.no_grad()
    def __iter__(self):
        while True:
            n = len(self.replay_memory)
            # with replacement is OK for off-policy
            yield from torch.randint(0, n, (16,)).tolist()
        

class PrioritizedReplaySampler(AbstractSampler):

    def __init__(self, replay_memory: "AbstractReplayMemory", alpha=0.65, beta_0=0.5, duration=150_000):
        super().__init__()
        self.replay_memory = replay_memory
        self.alpha = alpha
        self.beta_0 = beta_0
        self.duration = duration
        self.priorities = SumTree(size=self.replay_memory.max_length)
        self.random_sampler = RandomSampler(replay_memory=self.replay_memory)
        self.step = 0

    @torch.no_grad()
    def __len__(self) -> int:
        return len(self.replay_memory)
    
    @torch.no_grad()
    def __iter__(self):
        for _ in range(len(self)):
            if self.step >= self.duration:
                yield from self.random_sampler.__iter__()
            else:
                indices = self.priorities.sample(16)
                indices = torch.tensor(indices).long()
                yield from indices
        
    def compute_weights_from_indices(self, indices : torch.Tensor):
        beta = min(1, self.beta_0 + (1 - self.beta_0) * self.step / self.duration)
        weights : np.ndarray= (len(self) * self.priorities[indices] / (self.priorities.sum() + 1E-8)) ** (-beta)
        weights = weights / (weights.max() + 1E-6)
        return weights


    @torch.no_grad()
    def update_experiences(self, agent : "AbstractAgent", indices : torch.Tensor, td_errors : torch.Tensor = None, **kwargs):
        td_errors = td_errors.abs().cpu().numpy()
        self.priorities[indices.cpu().numpy()] = (td_errors + 1e-6) ** self.alpha

    @torch.no_grad()
    def store(self, agent: "AbstractAgent", **kwargs):
        assert isinstance(
            agent, AbstractValueAgent
        ), "PrioritizedReplaySampler can only be used with QAgents"

        experience = agent.replay_memory.experience_dataclass_generator(
            **{name : torch.as_tensor(value, dtype = agent.replay_memory.dtypes[name]) for name, value in kwargs.items()}
        )
        loss_inputs = agent.q_function.compute_loss_inputs(experience=experience)

        td_errors = agent.q_function.compute_td_errors(loss_inputs).abs().cpu().numpy()
        new_priorities = (td_errors + 1e-6) ** self.alpha 
        self.priorities.add(new_priorities)
        self.step += 1


# end of file: rl_agents\replay_memory\sampler.py

# path: rl_agents\service.py
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING
import torch
from enum import Enum

if TYPE_CHECKING:
    from rl_agents.agent import AbstractAgent


class AgentService(torch.nn.Module, ABC):
    def __init__(self, *args, **kwargs):
        torch.nn.Module.__init__(self, *args, **kwargs)


    def __update__(self, agent: "AbstractAgent"):
        self.update(agent=agent)
        for element in self.children():
            if isinstance(element, AgentService):
                element.__update__(agent=agent)
        
    def update(self, agent: "AbstractAgent"): ...


# end of file: rl_agents\service.py

# path: rl_agents\utils\__init__.py


# end of file: rl_agents\utils\__init__.py

# path: rl_agents\utils\collates.py
def do_nothing_collate(data):
    """
    As BaseReplayMemory implements __getitems__, the dataloader is able to call the experiences of the batch.
    Plus, the replay memories already output the correct format so collate_fn in not neccessary
    """
    return data

# end of file: rl_agents\utils\collates.py

# path: rl_agents\utils\mode.py
from contextlib import contextmanager
import torch

@contextmanager
def eval_mode(net : torch.nn.Module):
    '''Temporarily switch to evaluation mode.'''
    istrain = net.training
    try:
        net.eval()
        yield net
    finally:
        if istrain:
            net.train()

@contextmanager
def train_mode(net : torch.nn.Module):
    '''Temporarily switch to evaluation mode.'''
    iseval = not net.training
    try:
        net.train()
        yield net
    finally:
        if iseval:
            net.eval()

# end of file: rl_agents\utils\mode.py

# path: rl_agents\utils\noisynet.py
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
from __future__ import annotations

import math
from typing import Sequence

import torch
from torch import nn


class NoisyLinear(nn.Linear):
    """Noisy Linear Layer.

    Presented in "Noisy Networks for Exploration", https://arxiv.org/abs/1706.10295v3

    A Noisy Linear Layer is a linear layer with parametric noise added to the weights. This induced stochasticity can
    be used in RL networks for the agent's policy to aid efficient exploration. The parameters of the noise are learned
    with gradient descent along with any other remaining network weights. Factorized Gaussian
    noise is the type of noise usually employed.


    Args:
        in_features (int): input features dimension
        out_features (int): out features dimension
        bias (bool, optional): if ``True``, a bias term will be added to the matrix multiplication: Ax + b.
            Defaults to ``True``
        device (DEVICE_TYPING, optional): device of the layer.
            Defaults to ``"cpu"``
        dtype (torch.dtype, optional): dtype of the parameters.
            Defaults to ``None`` (default pytorch dtype)
        std_init (scalar, optional): initial value of the Gaussian standard deviation before optimization.
            Defaults to ``0.1``

    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        device: None = None,
        dtype: torch.dtype | None = None,
        std_init: float = 0.1,
    ):
        nn.Module.__init__(self)
        self.in_features = int(in_features)
        self.out_features = int(out_features)
        self.std_init = std_init

        self.weight_mu = nn.Parameter(
            torch.empty(
                out_features,
                in_features,
                device=device,
                dtype=dtype,
                requires_grad=True,
            )
        )
        self.weight_sigma = nn.Parameter(
            torch.empty(
                out_features,
                in_features,
                device=device,
                dtype=dtype,
                requires_grad=True,
            )
        )
        self.register_buffer(
            "weight_epsilon",
            torch.empty(out_features, in_features, device=device, dtype=dtype),
        )
        if bias:
            self.bias_mu = nn.Parameter(
                torch.empty(
                    out_features,
                    device=device,
                    dtype=dtype,
                    requires_grad=True,
                )
            )
            self.bias_sigma = nn.Parameter(
                torch.empty(
                    out_features,
                    device=device,
                    dtype=dtype,
                    requires_grad=True,
                )
            )
            self.register_buffer(
                "bias_epsilon",
                torch.empty(out_features, device=device, dtype=dtype),
            )
        else:
            self.bias_mu = None
        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self) -> None:
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        if self.bias_mu is not None:
            self.bias_mu.data.uniform_(-mu_range, mu_range)
            self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def reset_noise(self) -> None:
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        if self.bias_mu is not None:
            self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int | torch.Size | Sequence) -> torch.Tensor:
        if isinstance(size, int):
            size = (size,)
        x = torch.randn(*size, device=self.weight_mu.device)
        return x.sign().mul_(x.abs().sqrt_())

    @property
    def weight(self) -> torch.Tensor:
        return self.weight_mu + self.weight_sigma * self.weight_epsilon

    @property
    def bias(self) -> torch.Tensor | None:
        if self.bias_mu is not None:
            return self.bias_mu + self.bias_sigma * self.bias_epsilon
        else:
            return None



# end of file: rl_agents\utils\noisynet.py

# path: rl_agents\utils\sumtree.py
import numpy as np
from collections.abc import Iterable


epsilon = 1e-6  # Small constant to avoid zero‑probability issues

class SumTree:
    """Binary SumTree for Prioritized Experience Replay (PER).

    Each leaf stores a *non‑negative* priority p_i. Internal nodes store the sum
    of their two children. Thanks to this property, we can sample a leaf in
    O(log N) via a cumulative‑sum lookup.

    Parameters
    ----------
    size : int
        Maximum number of elements that can be stored (capacity). When the tree
        is full, `add()` overwrites in circular fashion (FIFO).
    """

    def __init__(self, size: int):
        if size <= 0:
            raise ValueError("size must be a strictly positive integer")

        self.size = int(size)
        self.length = 0          # Number of elements actually inserted

        # ----- Build layered array of sums (bottom → top) -----
        self.node_layers: list[np.ndarray] = [np.zeros(size, dtype=np.float32)]
        layer_size = size
        while layer_size > 1:
            layer_size = (layer_size + 1) // 2
            self.node_layers.append(np.zeros(layer_size, dtype=np.float32))

        self.n_layers = len(self.node_layers)

    # ---------------------------------------------------------------------
    # Internal helpers
    # ---------------------------------------------------------------------
    def _propagate(self, leaf_idx: int, change: float) -> None:
        """Propagate *change* from a leaf up to the root."""
        for layer in range(1, self.n_layers):
            leaf_idx //= 2
            self.node_layers[layer][leaf_idx] += change

    # ---------------------------------------------------------------------
    # Public API
    # ---------------------------------------------------------------------
    def add(self, value: float | Iterable[float]):
        """Insert a priority (or a batch of priorities) at the next position.

        If the capacity is reached, we overwrite in FIFO order.
        """
        if isinstance(value, Iterable):
            for v in value:
                self.add(v)
            return

        v = float(max(value, epsilon))  # Clamp to strictly positive
        leaf_idx = self.length % self.size
        self.length += 1

        change = v - self.node_layers[0][leaf_idx]
        self.node_layers[0][leaf_idx] = v
        self._propagate(leaf_idx, change)

    def __setitem__(self, idx: int | np.ndarray, value):
        """Update one or several priorities in‑place."""
        if isinstance(idx, (list, np.ndarray)):
            idx = np.asarray(idx, dtype=int)
            value = np.asarray(value, dtype=np.float32)
            if idx.shape != value.shape:
                raise ValueError("index and value must have the same shape")
            for i, v in zip(idx, value):
                self.__setitem__(int(i), float(v))
            return

        if idx >= self.length:
            raise IndexError(
                f"Cannot set value at index {idx} (current length = {self.length})")

        v = float(max(value, epsilon))
        change = v - self.node_layers[0][idx]
        self.node_layers[0][idx] = v
        self._propagate(idx, change)

    def __getitem__(self, idx: int | np.ndarray):
        return self.node_layers[0][idx]

    # ------------------------------------------------------------------
    # Sampling
    # ------------------------------------------------------------------
    def sum(self) -> float:
        """Return the total priority (value stored in the root)."""
        return float(self.node_layers[-1][0])

    def sample(self, batch_size: int) -> list[int]:
        """Sample *batch_size* leaf indices proportionally to their priorities.
        Raises ValueError if the tree is empty.
        """
        if self.length == 0:
            raise ValueError("Cannot sample from an empty SumTree")

        total = self.sum()
        if not np.isfinite(total) or total <= 0.0:
            raise ValueError(f"Invalid total priority: {total}")

        cumsums = np.random.rand(batch_size) * (total - epsilon)
        indices: list[int] = []

        for cs in cumsums:
            idx = 0  # Start from the root.
            for layer in range(self.n_layers - 1, 0, -1):
                left_idx = idx * 2
                left_sum = self.node_layers[layer - 1][left_idx]
                if cs < left_sum:
                    idx = left_idx
                else:
                    idx = left_idx + 1
                    cs -= left_sum
            # `idx` is now a leaf index.
            if idx >= self.length:
                # This can happen only if some of the last leaves were never
                # filled (length < capacity). Draw again.
                idx = np.random.randint(0, self.length)
            indices.append(idx)

        return indices

# end of file: rl_agents\utils\sumtree.py

# path: rl_agents\value_agents\__init__.py


# end of file: rl_agents\value_agents\__init__.py

# path: rl_agents\value_agents\double_q_net.py
from rl_agents.agent import AbstractAgent
from rl_agents.service import AgentService
from copy import deepcopy
import torch
from typing import Callable


class DoubleQNNProxy(AgentService):
    def __init__(
        self,
        q_net : 'AgentService',
        tau: int,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.tau = tau
        self.q_net: AgentService = q_net
        self.q_net_target: AgentService = deepcopy(q_net)
        self.q_net_target.requires_grad_(False)

        self._copy_weights()

    def forward(self, *args, **kwargs):
        if self.training:
            return self.q_net.forward(*args, **kwargs)
        return self.q_net_target.forward(*args, **kwargs)

    @torch.no_grad()
    def update(self, agent: AbstractAgent):
        if agent.step % self.tau == 0:
            self._copy_weights()

    def _copy_weights(self):
        self.q_net_target.load_state_dict(self.q_net.state_dict())

class SoftDoubleQNNProxy(DoubleQNNProxy):
    def __init__(
        self,
        q_net : 'AgentService',
        tau: int,
        **kwargs
    ):
        super().__init__(q_net=q_net, tau=tau)

    @torch.no_grad()
    def update(self, agent: AbstractAgent):
        target_net_state_dict = self.q_net_target.state_dict()
        net_state_dict = self.q_net.state_dict()
        for key in net_state_dict:
            target_net_state_dict[key] = net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)
        self.q_net_target.load_state_dict(target_net_state_dict)




# end of file: rl_agents\value_agents\double_q_net.py

# path: rl_agents\value_agents\dqn.py
from rl_agents.value_functions.dqn_function import DQNFunction
from rl_agents.policies.policy import AbstractPolicy
from rl_agents.value_agents.value_agent import AbstractValueAgent
from rl_agents.replay_memory.replay_memory import BaseReplayMemory, MultiStepReplayMemory
from rl_agents.replay_memory.sampler import AbstractSampler
from rl_agents.utils.collates import do_nothing_collate
import torch


class DQNAgent(AbstractValueAgent):
    def __init__(
        self,
        nb_env: int,
        policy: AbstractPolicy,
        q_function : DQNFunction,
        train_every : int,
        replay_memory : BaseReplayMemory,
        sampler : AbstractSampler,
        batch_size : int,
        optimizer : torch.optim.Optimizer
    ):

        torch.nn.Module.__init__(self)
        AbstractValueAgent.__init__(self, q_function=q_function, nb_env=nb_env, policy=policy)
        self.q_function = q_function
        self.train_every = train_every
        self.replay_memory = replay_memory
        self.sampler = sampler
        self.batch_size = batch_size
        self.optimizer = optimizer

        self.dataloader = torch.utils.data.DataLoader(
            dataset=self.replay_memory, 
            sampler=self.sampler, 
            collate_fn=do_nothing_collate, 
            batch_size=batch_size
        )
        self._dataloader_iter = iter(self.dataloader)
        assert isinstance(self.q_function, DQNFunction), "q_function must be from class DQNFunction, or inherit from it"

        if isinstance(self.replay_memory, MultiStepReplayMemory):
            self.q_function.gamma **= self.replay_memory.multi_step

    def store(self, **kwargs):
        assert self.training, "Cannot store any memory during eval."
        
        for key, value in kwargs.items():
            kwargs[key] = torch.as_tensor(value)
            if self.nb_env == 1: kwargs[key] = kwargs[key][None, ...] # Uniformize the shape, so first dim is always nb_env 
        
        self.replay_memory.store(agent=self, **kwargs)
        self.sampler.store(agent=self, **kwargs)


    def train_step_from_dataloader(self):
        try:
            experience = next(self._dataloader_iter)
        except StopIteration:
            self._dataloader_iter = iter(self.dataloader)
            experience = next(self._dataloader_iter)
        
        self.optimizer.zero_grad()

        q_loss = self.q_function.get_loss(agent=self, experience=experience)
        q_loss = self._apply_weights(q_loss, self.sampler.compute_weights_from_indices(experience.indices))

        q_loss = q_loss.mean()
        q_loss.backward()

        self.optimizer.step()

        return q_loss.item()

            
    def train_agent(self) -> float:
        super().train_agent()
        
        if self.step % self.train_every == 0 and len(self.replay_memory) > self.batch_size:
            # Training Q function
            loss = self.train_step_from_dataloader()
            return loss

    def _apply_weights(self, loss : torch.Tensor, weight : torch.Tensor | float |None):
        # Handle weights
        if weight is not None:
            weight = torch.as_tensor(weight, dtype=torch.float32)
            try: return loss * weight
            except BaseException as e: 
                raise ValueError(f"Could not apply weight from a loss of shape {loss.shape} and {weight.shape}. Initial Exception : {e}")
        return loss

# end of file: rl_agents\value_agents\dqn.py

# path: rl_agents\value_agents\noisy_net_strategy.py
from rl_agents.service import AgentService
from rl_agents.utils.noisynet import NoisyLinear
from rl_agents.utils.mode import train_mode
import torch
import torch.nn as nn

class NoisyNetProxy(AgentService):
    """
    In-place wrapper: replaces all nn.Linear layers in q_net by torchrl.modules.NoisyLinear.
    Keeps device/dtype and training mode; does NOT transfer weights (fresh noisy layers).
    """
    def __init__(self, q_net: AgentService, std_init: float = 0.1):
        super().__init__()
        self.q_net = q_net
        self.std_init = std_init
        self._patch(self.q_net)

    def _patch(self, module: nn.Module):
        for name, child in list(module.named_children()):
            if isinstance(child, nn.Linear) and not isinstance(child, NoisyLinear):
                noisy = NoisyLinear(
                    in_features=child.in_features,
                    out_features=child.out_features,
                    bias=(child.bias is not None),
                    std_init=self.std_init,
                    device=child.weight.device,
                    dtype=child.weight.dtype,
                )
                noisy.train(child.training)
                setattr(module, name, noisy)
            else:
                self._patch(child)

    def forward(self, *args, **kwargs):
        # Just delegate to the wrapped net
        return self.q_net(*args, **kwargs)

# end of file: rl_agents\value_agents\noisy_net_strategy.py

# path: rl_agents\value_agents\value_agent.py
from rl_agents.agent import AbstractAgent

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from rl_agents.policies.policy import AbstractPolicy
    from rl_agents.value_functions.q_function import AbstractQFunction

import torch
from abc import ABC, abstractmethod


class AbstractValueAgent(AbstractAgent, ABC):
    def __init__(self, q_function : 'AbstractQFunction', nb_env : int, policy : 'AbstractPolicy'):
        super().__init__(nb_env = nb_env, policy = policy)
        self.q_function = q_function
    ...
    # @abstractmethod
    # def Q(self, state: torch.Tensor): ...

    # @abstractmethod
    # def Q_a(self, state: torch.Tensor, action: torch.Tensor): ...

    # @abstractmethod
    # def compute_td_errors(self): ...

    # @abstractmethod
    # def compute_loss_inputs(self): ...

# end of file: rl_agents\value_agents\value_agent.py

# path: rl_agents\value_functions\distributional_dqn_function.py
from rl_agents.value_functions.dqn_function import DQNFunction
from rl_agents.replay_memory.replay_memory import ExperienceSample, Experience
from rl_agents.service import AgentService
from rl_agents.trainers.trainer import Trainer
from rl_agents.utils.mode import eval_mode
import torch
import matplotlib.pyplot as plt

class DistributionalLoss(torch.nn.modules.loss._Loss):
    def __init__(self):
        super().__init__()
        
    def forward(self, input : torch.Tensor, target : torch.Tensor):
        return - (target * (input + 1E-8).log()).sum(-1)

class DistributionalDQNFunction(DQNFunction):
    
    def __init__(self,
            nb_atoms : int,
            v_min : float,
            v_max : float,
            net : AgentService,
            gamma : float,
            loss_fn : torch.nn.modules.loss._Loss,
        ):
        super().__init__(net=net, gamma=gamma, loss_fn = loss_fn)
        self.nb_atoms = nb_atoms
        self.v_min = v_min
        self.v_max = v_max

        self._delta_atoms = (v_max - v_min) / (nb_atoms - 1)
        self.register_buffer("_atoms", torch.linspace(start = self.v_min, end = self.v_max, steps = self.nb_atoms, dtype = torch.float32))

    def compute_td_errors(self, loss_inputs : tuple[torch.Tensor, torch.Tensor]):
        y_pred, y_true = loss_inputs
        return self.get_value_from_atom_probs(y_true) - self.get_value_from_atom_probs(y_pred)
    
    def get_value_from_atom_probs(self, inputs):
        return (inputs * self._atoms).sum(dim = -1)

    def compute_loss_inputs(self, experience : Experience) -> None:
        batch_size = experience.state.size(0)
        y_pred = self.Q_a(state = experience.state, actions = experience.action, return_atoms= True)

        with torch.no_grad(), eval_mode(self):
            p_next = self.Q(state=experience.next_state, return_atoms= True) #[batch, nb_actions, nb_atoms]
            q_next = self.get_value_from_atom_probs(p_next) # [batch, nb_actions]
            a_star = torch.argmax(q_next, dim = 1, keepdim= True) #[batch]
            p_next = p_next.gather(1, a_star.unsqueeze(-1).expand(-1, 1, self.nb_atoms)).squeeze(1) # [batch, nb_atoms]


            # reward ([batch, 1]) + atoms([1, nb_atoms])
            T_z = torch.clip(
                experience.reward.unsqueeze(-1) + (1 - experience.done.to(p_next.dtype)).unsqueeze(-1)* self.gamma * self._atoms.unsqueeze(0), 
                min = self.v_min, max = self.v_max
            ) #[batch, nb_atoms]
            
            b = (T_z - self.v_min) / self._delta_atoms  #[batch, nb_atoms]
            l = torch.floor(b).long().clamp_max(self.nb_atoms - 1) #[batch, nb_atoms]
            u = torch.ceil(b).long().clamp_max(self.nb_atoms - 1) #[batch, nb_atoms]
    
            m = torch.zeros(batch_size, self.nb_atoms, dtype=torch.float32, device=p_next.device)# [batch, nb_atoms]

            m.scatter_add_(1, l, (p_next * (u.float() - b)).float())
            m.scatter_add_(1, u, (p_next * (b - l.float())).float())
            y_true = m

        return (y_pred, y_true) #both : [batch, nb_atoms]

    def Q(self, state: torch.Tensor, return_atoms = False) -> torch.Tensor:
        q_logits = self.net.forward(state)
        q_prob = torch.softmax(q_logits, dim=-1) #[batch/nb_env, nb_actions, nb_atoms]
        return q_prob if return_atoms else self.get_value_from_atom_probs(q_prob)
    
    def Q_a(
        self, state: torch.Tensor, actions: torch.Tensor, return_atoms = False
    ) -> torch.Tensor:
        # actions : [batch]
        q_values = self.Q(state, return_atoms=return_atoms) #[batch/nb_env, nb_actions, nb_atoms] if return_atoms is True else [batch/nb_env, nb_actions]
        batch_idx = torch.arange(q_values.size(0), device=q_values.device)
        return q_values[batch_idx, actions.long()]  # shape [B] ou [B, N]
    
    def plot_atoms_distributions(self, replay_memory, n_samples: int = 10_000, max_batch: int = 512):
        if len(replay_memory) < n_samples:
            print(f"ReplayMemory (len = {len(replay_memory)}) does not provide enough samples (n_samples = {n_samples}).")
            return

        nb_actions, nb_atoms = self.policy.action_space.n, self.nb_atoms
        sum_p = torch.zeros(nb_actions, nb_atoms, device=self.device)
        seen = 0

        with torch.no_grad(), eval_mode(self):
            while seen < n_samples:
                batch_sz = min(max_batch, n_samples - seen)
                batch = replay_memory.sample(batch_sz)
                if batch is None:          # pas assez de données stockées
                    break

                states = batch["state"]
                p = self.Q(states, return_atoms=True)   # (B, A, N)
                sum_p += p.sum(dim=0)                        # accumulate sur B
                seen += p.size(0)

            if seen == 0:
                print("Pas assez de transitions pour échantillonner.")
                return

            mean_p = sum_p / seen                           # (A, N)

        # ---------- Plot ----------
        z = self._atoms.cpu().numpy()
        plt.figure(figsize=(8, 4))
        for a in range(nb_actions):
            plt.plot(z, mean_p[a].cpu().numpy(),
                    label=f"action {a}", linewidth=2)
        plt.title(f"Distribution moyenne sur {seen} transitions")
        plt.xlabel("z (valeurs atomiques)")
        plt.ylabel("Probabilité")
        plt.grid(True, linestyle="--", alpha=0.4)
        plt.legend()
        plt.tight_layout()
        plt.show()
        self.train()


# end of file: rl_agents\value_functions\distributional_dqn_function.py

# path: rl_agents\value_functions\dqn_function.py
from rl_agents.value_functions.q_function import AbstractQFunction
from rl_agents.value_functions.v_function import AbstractVFunction
from rl_agents.replay_memory.replay_memory import AbstractReplayMemory, ExperienceSample, Experience
from rl_agents.service import AgentService
from rl_agents.utils.mode import eval_mode, train_mode

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from rl_agents.agent import AbstractAgent

from typing import Callable
import torch

    
class DVNFunction(AbstractVFunction):
    def __init__(self,
            net : AgentService,
            gamma : float,
            loss_fn : torch.nn.modules.loss._Loss,
        ):
        torch.nn.Module.__init__(self)
        self.net = net
        self.gamma = gamma
        self.loss_fn = loss_fn
        loss_fn.reduction= "none"

    
    def V(self, state: torch.Tensor, compute_target = False) -> torch.Tensor:
        # state : [batch/nb_env, state_shape ...]
        if compute_target:
            with eval_mode(self): return self.net.forward(state).squeeze(-1)
        return self.net.forward(state).squeeze(-1)

        # Return Q Values : [batch/nb_env]
    

    def compute_td_errors(self, loss_inputs : tuple[torch.Tensor, torch.Tensor]):
        y_pred, y_true = loss_inputs
        return (y_true - y_pred).abs()


    def compute_loss_inputs(self, experience : Experience) -> None:
        y_pred = self.V(experience.state)  # [batch/nb_env]
        with torch.no_grad():
            y_true = experience.reward + (1 - experience.done.float()) * self.gamma * self.V(experience.next_state, compute_target=True) # is meant to predict the end of the mathematical sequence
        return (y_pred, y_true)

    def get_loss(self, agent : "AbstractAgent", experience : ExperienceSample) -> float:
        # state: torch.Tensor,  # [batch, state_shape ...] obtained at t
        # action: torch.Tensor,  # [batch] obtained at t+multi_steps
        # reward: torch.Tensor,  # [batch] obtained between t+1 and t+multi_step (then summed up using discounted sum)
        # next_state: torch.Tensor,  # [batch, state_shapes ...] obtained at t+multi_steps
        # done: torch.Tensor,  # [batch] obtained at t+multi_steps
        # weight: torch.Tensor = None,
        # replay_memory_callbacks: Callable

        loss_inputs = self.compute_loss_inputs(experience)

        loss :torch.Tensor = self.loss_fn(*loss_inputs)
    
        with torch.no_grad():
            agent.sampler.update_experiences(
                agent = agent, indices = experience.indices, td_errors = self.compute_td_errors(loss_inputs=loss_inputs)
            )

        return loss

class DQNFunction(DVNFunction, AbstractQFunction):
    def __init__(self,
            net : AgentService,
            gamma : float,
            loss_fn : torch.nn.modules.loss._Loss
        ):
        super().__init__(net = net, gamma= gamma, loss_fn=loss_fn)

    def Q(self, state: torch.Tensor) -> torch.Tensor:
        # state : [batch/nb_env, state_shape ...]
        return self.net.forward(state)
        # Return Q Values : [batch/nb_env, nb_actions]

    def Q_a(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        q_values = self.Q(state)
        # print("Test2 : ", actions.shape, q_values.shape)
        return q_values.gather(dim=1, index=action.long().unsqueeze(1)).squeeze(1)
    
    def V(self, state: torch.Tensor, compute_target = False) -> torch.Tensor:
        if not compute_target: 
            return torch.amax(self.Q(state=state), dim = -1) # max_{a} ( Q_θ(s_t, a) )
        # When we compute target :
        #  Q_θ'(s_t, argmax_{a} (Q_θ(s_t, a)))
        best_action = torch.argmax(self.Q(state=state), dim = 1)
        with eval_mode(self):
            return self.Q_a(state=state, action=best_action) # We use Q'
        
    
    def compute_loss_inputs(self, experience : ExperienceSample) -> None:
        y_pred = self.Q_a(experience.state, experience.action)  # [batch/nb_env]

        with torch.no_grad():
            y_true = experience.reward + (1 - experience.done.float()) * self.gamma * self.V(experience.next_state, compute_target=True)  # is meant to predict the end of the mathematical sequence
        return y_pred, y_true

# end of file: rl_agents\value_functions\dqn_function.py

# path: rl_agents\value_functions\q_function.py
from rl_agents.agent import AbstractAgent
from rl_agents.service import AgentService
from rl_agents.value_functions.v_function import AbstractVFunction
from abc import ABC, abstractmethod
from typing import Callable
import torch

class AbstractQFunction(AbstractVFunction):
    @abstractmethod
    def Q(self, state: torch.Tensor) -> torch.Tensor: ...

    @abstractmethod
    def Q_a(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor: ...


# end of file: rl_agents\value_functions\q_function.py

# path: rl_agents\value_functions\v_function.py
from rl_agents.agent import AbstractAgent
from rl_agents.service import AgentService
from rl_agents.trainers.trainable import Trainable

from abc import ABC, abstractmethod
from typing import Callable
import torch


class AbstractVFunction(AgentService, ABC):
    @abstractmethod
    def V(self, state: torch.Tensor) -> torch.Tensor: ...

    _gamma = None
    @property
    def gamma(self) -> int:
        if self._gamma is None: raise AttributeError(f"{self.__class__.__name__}.gamma is not set")
        return self._gamma
    @gamma.setter
    def gamma(self, val): self._gamma = val

# end of file: rl_agents\value_functions\v_function.py
