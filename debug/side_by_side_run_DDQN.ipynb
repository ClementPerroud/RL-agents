{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e689068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8417f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "# COMMON\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "device = \"cpu\"\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_DIM = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 5000\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "MEMORY_SIZE = int(1E5)\n",
    "\n",
    "# MY IMPLEMENTATION\n",
    "TRAIN_EVERY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995fa7d",
   "metadata": {},
   "source": [
    "## Common implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0cffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_observations, HIDDEN_DIM), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN_DIM, HIDDEN_DIM), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN_DIM, HIDDEN_DIM), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN_DIM, n_actions)\n",
    "    )\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0107e9",
   "metadata": {},
   "source": [
    "## Set up torch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f8bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598b9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.0.weight Parameter containing:\n",
      "tensor([[-0.0105, -0.0834,  0.2556,  ..., -0.0589,  0.1923, -0.0328],\n",
      "        [-0.1855,  0.2942, -0.0101,  ...,  0.2289,  0.0792, -0.2281],\n",
      "        [-0.0905, -0.2371,  0.3186,  ..., -0.3509,  0.3200, -0.0291],\n",
      "        ...,\n",
      "        [-0.1738, -0.2934, -0.1607,  ...,  0.2083,  0.0533,  0.2644],\n",
      "        [-0.3118, -0.1897, -0.2179,  ...,  0.3258, -0.1586, -0.1711],\n",
      "        [-0.1981, -0.1624,  0.1675,  ...,  0.3465, -0.0034, -0.1492]],\n",
      "       requires_grad=True)\n",
      "layer.0.bias Parameter containing:\n",
      "tensor([ 0.3034,  0.1843,  0.2829,  0.3159, -0.0299,  0.1589,  0.2006,  0.1392,\n",
      "         0.3504, -0.3060, -0.2733,  0.3107, -0.2974,  0.1568,  0.2277,  0.1744,\n",
      "         0.0953,  0.3125, -0.3295,  0.1196,  0.3253, -0.0090,  0.1299, -0.3176,\n",
      "        -0.1753, -0.2778,  0.2537,  0.2408, -0.2342,  0.0569,  0.0307, -0.1752,\n",
      "         0.1397,  0.1097, -0.2283, -0.1170,  0.2929,  0.0330, -0.1557,  0.1814,\n",
      "         0.1345, -0.2691,  0.1081,  0.0055,  0.1841,  0.2820,  0.2953, -0.0954,\n",
      "         0.2354,  0.2042,  0.2082,  0.0521, -0.0495,  0.1214,  0.2886,  0.1333,\n",
      "         0.0483, -0.0571, -0.0364,  0.1608,  0.2627, -0.1945, -0.2506,  0.2064,\n",
      "         0.1969, -0.2187, -0.3154, -0.0153,  0.0905, -0.0710,  0.1542, -0.0683,\n",
      "         0.3025, -0.0293,  0.2185, -0.1744,  0.2980, -0.1281, -0.1217,  0.2987,\n",
      "         0.0017,  0.1762,  0.0194,  0.0435,  0.3218,  0.1331, -0.0474,  0.1287,\n",
      "         0.1353,  0.2535,  0.0004,  0.0223, -0.0044, -0.0007, -0.1158, -0.0053,\n",
      "        -0.3296,  0.3140,  0.2946,  0.1583,  0.3073,  0.0567,  0.0085,  0.3128,\n",
      "        -0.0014, -0.3037, -0.0005, -0.1194,  0.0325, -0.0545,  0.2684, -0.0650,\n",
      "         0.1072,  0.0291,  0.1193,  0.2831,  0.0380, -0.1349, -0.1255,  0.1785,\n",
      "         0.1968,  0.2232, -0.0760, -0.2182, -0.1441, -0.1219, -0.0351,  0.2091],\n",
      "       requires_grad=True)\n",
      "layer.2.weight Parameter containing:\n",
      "tensor([[-0.0076,  0.0523, -0.0064,  ..., -0.0159, -0.0863, -0.0364],\n",
      "        [ 0.0202, -0.0673, -0.0280,  ...,  0.0706, -0.0863,  0.0465],\n",
      "        [ 0.0605,  0.0497, -0.0772,  ..., -0.0757,  0.0209,  0.0414],\n",
      "        ...,\n",
      "        [ 0.0224,  0.0353, -0.0695,  ...,  0.0853,  0.0218, -0.0586],\n",
      "        [ 0.0820, -0.0526,  0.0475,  ..., -0.0497, -0.0115,  0.0837],\n",
      "        [-0.0106,  0.0351, -0.0297,  ..., -0.0880,  0.0118,  0.0010]],\n",
      "       requires_grad=True)\n",
      "layer.2.bias Parameter containing:\n",
      "tensor([ 2.2136e-02,  4.1616e-02, -1.5928e-02,  1.5757e-02,  6.5510e-02,\n",
      "         5.9328e-02,  3.9937e-02,  8.5238e-02, -2.9944e-02,  5.8594e-02,\n",
      "        -1.9055e-02, -2.9391e-02, -6.6326e-02,  6.8714e-02,  3.3607e-02,\n",
      "         4.8519e-03, -8.6932e-02,  1.4787e-02, -9.4492e-04, -5.4888e-02,\n",
      "        -5.4700e-02, -6.9784e-02,  6.7907e-03, -8.1196e-03,  8.6494e-02,\n",
      "        -5.1151e-02, -4.7673e-02,  1.6640e-02,  5.2766e-02, -2.1403e-02,\n",
      "        -4.0100e-02,  8.8009e-02,  2.1721e-02, -5.0805e-02, -2.5040e-02,\n",
      "        -4.1789e-02, -4.5190e-02,  3.6025e-02, -1.1114e-02, -7.4305e-02,\n",
      "         1.0985e-02, -3.8836e-02, -6.9079e-02, -3.4229e-02, -7.2622e-03,\n",
      "         5.6572e-02, -6.2543e-02,  7.3665e-02,  1.6247e-02,  8.5147e-02,\n",
      "        -4.1438e-02,  7.9252e-02, -3.1361e-02,  2.6084e-02,  4.1530e-02,\n",
      "         1.2891e-02,  2.0695e-02, -7.6436e-02, -6.4829e-02, -8.0949e-03,\n",
      "        -8.4542e-02, -6.5173e-02, -4.6651e-02, -1.6426e-02, -3.6152e-02,\n",
      "         1.6029e-02, -7.1047e-02, -7.8502e-02, -3.1419e-02,  1.2872e-02,\n",
      "        -4.3092e-02,  5.0777e-02, -4.4920e-03,  4.2137e-02, -6.6487e-02,\n",
      "         3.8773e-02,  5.1336e-03, -7.9312e-03, -2.0056e-02, -2.2756e-02,\n",
      "         4.0350e-02, -5.1469e-02, -8.6573e-02,  5.7153e-04,  1.4803e-02,\n",
      "         3.8127e-02,  4.4192e-02,  3.0564e-02, -6.2298e-02, -8.1554e-05,\n",
      "         6.2915e-02, -8.2519e-02, -3.9090e-02,  4.2800e-02,  6.9978e-02,\n",
      "        -7.6159e-03, -3.3026e-02, -5.2844e-02, -1.9764e-03, -2.2485e-02,\n",
      "        -7.5440e-02, -8.4121e-02,  6.9251e-02,  1.9758e-02,  4.5559e-03,\n",
      "         4.4837e-02, -4.2871e-02,  1.4232e-02,  2.2376e-02,  4.3509e-02,\n",
      "         3.2363e-02,  2.1232e-02,  3.6670e-02,  7.6840e-02, -3.0819e-03,\n",
      "         1.3535e-02, -1.8189e-02, -5.8748e-02,  2.0121e-02,  5.6374e-02,\n",
      "         7.6347e-02, -1.5642e-02, -6.2956e-02,  1.3761e-02, -7.2881e-02,\n",
      "         6.9566e-02,  7.0998e-02,  8.5960e-02], requires_grad=True)\n",
      "layer.4.weight Parameter containing:\n",
      "tensor([[-0.0166,  0.0467, -0.0237,  ...,  0.0301,  0.0833,  0.0294],\n",
      "        [ 0.0851, -0.0373, -0.0659,  ..., -0.0086,  0.0411,  0.0115],\n",
      "        [-0.0651, -0.0054,  0.0688,  ..., -0.0260, -0.0152,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0204, -0.0208, -0.0201,  ...,  0.0058, -0.0065, -0.0480],\n",
      "        [-0.0052,  0.0393, -0.0349,  ..., -0.0551, -0.0081,  0.0260],\n",
      "        [-0.0672,  0.0636, -0.0508,  ..., -0.0687, -0.0667,  0.0858]],\n",
      "       requires_grad=True)\n",
      "layer.4.bias Parameter containing:\n",
      "tensor([-0.0427, -0.0124, -0.0583, -0.0735,  0.0603, -0.0425,  0.0675,  0.0458,\n",
      "         0.0653,  0.0426, -0.0512,  0.0828,  0.0873,  0.0282,  0.0388,  0.0798,\n",
      "         0.0483, -0.0475, -0.0793,  0.0483,  0.0183,  0.0386,  0.0113,  0.0110,\n",
      "        -0.0504,  0.0032, -0.0634, -0.0797,  0.0107, -0.0367, -0.0488, -0.0646,\n",
      "         0.0782,  0.0862, -0.0493, -0.0268, -0.0344,  0.0469, -0.0872, -0.0354,\n",
      "         0.0115, -0.0727,  0.0410, -0.0474, -0.0035,  0.0239,  0.0165, -0.0024,\n",
      "        -0.0262, -0.0694, -0.0231,  0.0175,  0.0267, -0.0465,  0.0859,  0.0103,\n",
      "         0.0133,  0.0440, -0.0699, -0.0869, -0.0570, -0.0254, -0.0049,  0.0147,\n",
      "         0.0824,  0.0298, -0.0355,  0.0023,  0.0863,  0.0473,  0.0353,  0.0518,\n",
      "         0.0021, -0.0365,  0.0718, -0.0866, -0.0615,  0.0823,  0.0331, -0.0715,\n",
      "         0.0603,  0.0277,  0.0413,  0.0326,  0.0233,  0.0267,  0.0791,  0.0321,\n",
      "        -0.0054, -0.0188,  0.0609,  0.0677, -0.0011,  0.0245, -0.0196, -0.0860,\n",
      "         0.0032, -0.0240, -0.0828,  0.0345,  0.0197, -0.0731,  0.0215, -0.0399,\n",
      "        -0.0102, -0.0343,  0.0335, -0.0639,  0.0082, -0.0182, -0.0555, -0.0569,\n",
      "         0.0659,  0.0662, -0.0796,  0.0671,  0.0530,  0.0562, -0.0445, -0.0241,\n",
      "        -0.0021,  0.0571,  0.0312,  0.0354,  0.0855, -0.0581, -0.0300,  0.0394],\n",
      "       requires_grad=True)\n",
      "layer.6.weight Parameter containing:\n",
      "tensor([[ 1.1405e-04, -1.7839e-03,  3.6144e-03,  8.2527e-02, -1.1998e-02,\n",
      "          4.6339e-02, -7.4711e-02,  2.9777e-02,  6.5934e-03, -5.3864e-02,\n",
      "         -5.0344e-03, -5.0276e-02, -6.6498e-02, -4.8888e-02, -7.6539e-02,\n",
      "         -8.1645e-02, -1.0821e-02,  5.4312e-02,  5.9703e-02,  6.1399e-02,\n",
      "         -6.4750e-02, -7.4553e-02, -4.8244e-03,  3.4276e-02, -1.3831e-02,\n",
      "          8.7036e-02,  2.6562e-02, -1.7632e-02,  8.7199e-03, -4.0436e-02,\n",
      "         -2.4659e-02,  7.2451e-02,  1.6445e-02,  7.3632e-02, -6.4286e-02,\n",
      "         -6.5613e-03,  5.0882e-02,  4.8825e-02, -8.1263e-03, -3.7809e-02,\n",
      "         -7.4683e-02, -7.7573e-02,  5.8011e-02, -8.7647e-02, -5.2331e-02,\n",
      "         -5.5124e-02, -7.7556e-02,  3.8911e-02, -7.2869e-02, -7.1781e-02,\n",
      "          7.4726e-02, -7.5554e-02, -7.2915e-03, -4.0248e-02, -1.7859e-02,\n",
      "         -1.6605e-02, -3.7904e-03,  5.1042e-02, -1.0864e-02, -6.2703e-03,\n",
      "          3.1604e-03, -6.2954e-02,  8.1476e-02,  6.2190e-02, -9.1309e-03,\n",
      "         -2.2610e-02,  6.9048e-02,  4.3776e-03, -8.6666e-02,  6.2847e-02,\n",
      "          6.9501e-02, -7.5177e-02,  2.8080e-02,  8.4491e-02,  8.0590e-02,\n",
      "          6.3560e-02,  5.5106e-02, -6.0566e-02,  1.3064e-02,  9.6896e-04,\n",
      "          6.6918e-02, -4.6072e-02,  6.3372e-03, -5.4609e-02, -6.8882e-02,\n",
      "         -6.9797e-02,  6.8383e-02,  8.0728e-02,  1.4888e-02, -3.8248e-02,\n",
      "         -5.6278e-02,  7.4259e-02, -1.5639e-02, -3.3251e-02,  5.1567e-02,\n",
      "         -4.7454e-02, -8.6386e-02,  6.8324e-02,  5.8210e-02, -3.1814e-02,\n",
      "         -5.7936e-02,  5.2288e-02, -1.0631e-02,  3.6292e-02,  1.6167e-02,\n",
      "         -3.8760e-02,  5.0792e-02,  2.4952e-02, -8.5133e-02,  3.3407e-02,\n",
      "         -1.6223e-02, -1.4595e-02,  4.9326e-02,  6.1915e-02, -3.0390e-02,\n",
      "          8.0373e-03,  5.3299e-03,  6.5592e-02, -3.5956e-02,  7.3590e-03,\n",
      "          4.0578e-03,  8.1222e-02, -1.6778e-02,  8.5905e-02,  5.2855e-03,\n",
      "         -2.4696e-02, -4.7979e-02,  2.9416e-02],\n",
      "        [ 4.2958e-02, -8.5945e-02,  2.3870e-02, -8.4097e-02,  7.7388e-02,\n",
      "         -3.3111e-02,  5.8958e-02,  2.3953e-03, -3.1436e-02, -3.0671e-02,\n",
      "         -5.3128e-02, -1.6528e-02, -6.6820e-02, -6.8342e-02,  5.1692e-02,\n",
      "          8.2900e-02,  2.0612e-02,  8.7016e-02,  1.1273e-02,  5.7336e-02,\n",
      "         -1.0287e-02,  7.7715e-02,  2.9128e-02, -3.1362e-02,  8.2858e-02,\n",
      "         -2.5491e-02, -5.2869e-02, -2.3684e-02,  3.1778e-02, -5.6647e-02,\n",
      "          4.0456e-02, -7.3562e-02,  5.9963e-02, -4.9289e-04, -7.8229e-02,\n",
      "          1.6072e-02,  4.0192e-02,  1.0072e-02, -7.4159e-03, -5.2441e-02,\n",
      "         -3.8619e-02,  5.5121e-02,  6.4392e-02, -3.6212e-02, -1.3939e-02,\n",
      "         -8.2027e-02,  3.7384e-02, -4.7097e-03,  1.8777e-02, -4.8274e-02,\n",
      "          6.8269e-03, -5.0239e-02,  4.4842e-02, -4.3727e-02,  2.6144e-02,\n",
      "          6.9560e-02,  6.6675e-02, -1.6378e-02,  8.3557e-02,  2.2954e-02,\n",
      "         -1.9638e-02,  2.2887e-02,  7.3091e-03,  3.7857e-02,  2.6977e-02,\n",
      "         -1.4970e-02, -6.2604e-02, -3.6152e-02,  4.6506e-02,  4.8563e-02,\n",
      "          4.5790e-03,  7.6842e-02, -1.3658e-02,  7.1176e-02, -3.0991e-02,\n",
      "          7.1318e-02,  1.7185e-03,  3.4600e-03, -3.7360e-02, -5.7163e-02,\n",
      "         -4.4932e-02, -5.1414e-03,  8.0157e-02,  1.5989e-03, -4.3205e-02,\n",
      "          1.5015e-02,  6.4695e-02, -2.8605e-02,  3.7632e-02, -2.7735e-02,\n",
      "         -1.6827e-02, -4.1235e-02, -8.8705e-03, -4.7191e-02,  4.6914e-02,\n",
      "         -5.7266e-02,  7.8099e-02, -2.0290e-02,  6.0583e-02,  7.6619e-02,\n",
      "          3.6340e-02,  8.0911e-02,  3.5924e-02,  8.8191e-02, -1.7139e-02,\n",
      "          4.1635e-02,  1.0564e-02,  3.6007e-02,  1.1382e-02,  6.2794e-02,\n",
      "         -7.9622e-02, -8.2823e-02, -7.9711e-02, -5.6452e-03,  2.1397e-02,\n",
      "          1.7099e-02, -3.4873e-02,  1.5856e-02, -1.6472e-02, -3.5714e-02,\n",
      "          7.7371e-02,  5.1784e-02,  2.5197e-02, -4.5560e-02,  6.6334e-02,\n",
      "         -8.6729e-02, -6.6224e-03, -3.4042e-02],\n",
      "        [ 2.4719e-02, -5.1703e-02, -2.6492e-02,  8.2139e-02, -4.5394e-02,\n",
      "          1.4371e-02,  2.2248e-02, -5.8927e-02, -1.4807e-02, -2.0114e-02,\n",
      "         -2.5772e-04,  4.5106e-02, -4.3509e-02,  6.3352e-02, -8.0490e-02,\n",
      "         -5.4190e-02,  6.5770e-02,  1.7479e-02,  2.1719e-02,  6.8736e-02,\n",
      "          5.0623e-02,  4.9549e-03, -3.1315e-02,  7.0073e-02, -8.8192e-02,\n",
      "          6.8719e-02,  2.3866e-02,  4.4038e-02, -9.6229e-03, -5.8052e-02,\n",
      "          2.4478e-02,  9.9795e-03,  2.1306e-02,  3.5575e-02, -7.3885e-02,\n",
      "         -2.9222e-03,  8.2048e-02,  2.3470e-02, -6.1583e-02, -5.6979e-02,\n",
      "         -2.6452e-02,  7.5640e-02, -2.7828e-02,  3.6529e-02, -2.6320e-02,\n",
      "         -3.7900e-02,  5.3240e-02, -6.6592e-02,  2.9896e-02, -4.7777e-02,\n",
      "         -7.0631e-02,  5.8335e-02, -6.0482e-02, -7.8198e-02,  4.6009e-02,\n",
      "         -7.5639e-02,  7.6862e-02, -6.8125e-02,  1.2288e-02, -3.9586e-02,\n",
      "          3.3088e-02,  6.4054e-02,  3.5422e-02,  2.5413e-02, -4.7893e-02,\n",
      "         -4.9090e-02, -4.4685e-02,  4.4758e-02, -2.1576e-02,  7.0546e-02,\n",
      "          6.8909e-02,  1.7020e-02, -7.0122e-02,  7.6244e-03, -2.0094e-02,\n",
      "         -3.2701e-02,  5.6836e-02, -5.9305e-02,  3.0224e-02, -6.8151e-02,\n",
      "         -5.5552e-02, -2.3988e-02,  6.8203e-02,  8.1306e-02,  1.3700e-03,\n",
      "         -7.2552e-02, -3.7515e-02, -1.5852e-02,  8.5082e-02, -7.7957e-02,\n",
      "          1.7082e-02, -8.5036e-02, -7.5815e-03,  3.2283e-02, -6.9901e-02,\n",
      "          7.8446e-02,  1.8544e-04,  5.1789e-02, -9.2929e-03, -8.2636e-02,\n",
      "         -7.9641e-03, -4.8021e-02,  7.1748e-02,  5.9084e-02, -7.3375e-02,\n",
      "         -5.2466e-02,  8.5606e-02,  8.4576e-02,  6.9245e-02, -1.6461e-03,\n",
      "         -3.1162e-02, -6.4974e-02,  4.3341e-02, -8.6265e-02,  1.2657e-02,\n",
      "          4.2870e-03, -3.7623e-02,  2.2133e-02, -1.1673e-02, -4.0248e-02,\n",
      "          1.1283e-02,  8.0181e-02,  2.1293e-02,  2.5607e-02, -5.4555e-03,\n",
      "         -3.8707e-02,  7.6356e-02, -5.2521e-03],\n",
      "        [-8.4766e-02, -3.2769e-02,  5.0676e-02, -3.6136e-02,  4.8891e-02,\n",
      "         -7.2538e-02, -7.2495e-02, -5.6757e-02,  7.1831e-02, -2.5504e-03,\n",
      "         -2.5810e-02,  7.9356e-02, -2.4791e-02, -6.2759e-03,  3.4417e-02,\n",
      "          7.5965e-02, -5.3233e-02, -4.0531e-02,  5.7726e-02, -1.2249e-02,\n",
      "          6.2271e-02,  7.0040e-02,  4.9353e-02,  2.0499e-02,  6.5325e-02,\n",
      "          8.7793e-02, -8.6300e-02, -8.8957e-03, -6.4705e-02, -3.2513e-02,\n",
      "         -3.5270e-02, -3.7182e-02,  2.6412e-02, -2.9850e-02, -6.1484e-02,\n",
      "          4.0866e-02,  6.6407e-02, -3.2713e-02,  7.9893e-02, -6.0029e-02,\n",
      "          4.9745e-02, -5.0740e-02, -5.7973e-02,  5.2053e-02,  2.1983e-02,\n",
      "          4.9969e-02, -4.6316e-02,  2.5879e-02,  4.5945e-03,  2.0209e-03,\n",
      "          2.6607e-02, -7.7011e-02,  1.3218e-02,  2.5283e-02, -1.5147e-02,\n",
      "          1.5274e-02,  3.5145e-02,  7.2887e-03,  5.8006e-02,  4.6993e-02,\n",
      "         -8.8132e-02,  3.8375e-02,  5.2009e-02,  6.3144e-05,  2.3924e-02,\n",
      "          7.9259e-02,  6.5436e-03,  6.6430e-03,  5.8798e-02, -7.6525e-02,\n",
      "         -3.0247e-03, -3.7258e-02,  4.0848e-02, -7.4280e-02,  6.6091e-02,\n",
      "          5.0553e-02,  7.5320e-02,  5.5294e-02,  4.2759e-02, -1.8527e-02,\n",
      "          1.4820e-02, -7.9859e-02, -4.1802e-02, -4.5960e-02, -5.6511e-02,\n",
      "          3.7597e-02, -4.2590e-02,  7.0767e-02, -3.0317e-02,  2.0542e-02,\n",
      "          2.3728e-02, -5.2990e-02, -6.3945e-02,  4.4099e-02,  2.2490e-02,\n",
      "         -2.1280e-02,  3.1797e-02, -1.3832e-02,  6.6753e-02, -6.4382e-02,\n",
      "         -2.8553e-02,  4.3947e-02, -3.1153e-02,  8.8274e-02, -2.4707e-02,\n",
      "          2.5761e-02,  8.3994e-02, -8.0468e-02,  1.8607e-02,  3.7331e-02,\n",
      "          4.2796e-03,  6.7245e-02, -1.8452e-02,  5.4123e-02, -5.3739e-02,\n",
      "          2.8566e-02,  9.5482e-03,  2.3917e-02, -3.8977e-03, -7.8558e-02,\n",
      "         -5.0481e-02, -6.1948e-02, -6.7156e-02,  7.8542e-02,  6.2416e-02,\n",
      "         -5.3889e-02, -5.8834e-02,  8.6302e-02]], requires_grad=True)\n",
      "layer.6.bias Parameter containing:\n",
      "tensor([ 0.0499,  0.0340, -0.0431,  0.0129], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "metric_to_plot = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(metric_to_plot, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af7cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c00156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   0 - Steps :  103 | Total Rewards : -149.93 | Loss : nan | Epsilon :  0.88 | Agent Step : 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cl√©ment\\AppData\\Local\\Temp\\ipykernel_12364\\3034668483.py:49: RuntimeWarning: Mean of empty slice.\n",
      "  episode_loss = np.array(episode_losses).mean()\n",
      "c:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1 - Steps :   94 | Total Rewards :  -95.88 | Loss : 1.82e+00 | Epsilon :  0.87 | Agent Step : 197\n",
      "Episode   2 - Steps :   96 | Total Rewards :   17.08 | Loss : 1.71e+00 | Epsilon :  0.85 | Agent Step : 293\n",
      "Episode   3 - Steps :  131 | Total Rewards : -170.59 | Loss : 1.91e+00 | Epsilon :  0.83 | Agent Step : 424\n",
      "Episode   4 - Steps :  113 | Total Rewards : -266.90 | Loss : 1.47e+00 | Epsilon :  0.81 | Agent Step : 537\n",
      "Episode   5 - Steps :   96 | Total Rewards :  -53.20 | Loss : 1.58e+00 | Epsilon :  0.80 | Agent Step : 633\n",
      "Episode   6 - Steps :   96 | Total Rewards :    4.72 | Loss : 1.62e+00 | Epsilon :  0.78 | Agent Step : 729\n",
      "Episode   7 - Steps :  114 | Total Rewards : -146.69 | Loss : 1.73e+00 | Epsilon :  0.77 | Agent Step : 843\n",
      "Episode   8 - Steps :   79 | Total Rewards :  -59.77 | Loss : 1.63e+00 | Epsilon :  0.76 | Agent Step : 922\n",
      "Episode   9 - Steps :  104 | Total Rewards :  -75.78 | Loss : 1.82e+00 | Epsilon :  0.74 | Agent Step : 1026\n",
      "Episode  10 - Steps :  104 | Total Rewards :  -96.14 | Loss : 1.82e+00 | Epsilon :  0.73 | Agent Step : 1130\n",
      "Episode  11 - Steps :   94 | Total Rewards :  -93.47 | Loss : 1.93e+00 | Epsilon :  0.72 | Agent Step : 1224\n",
      "Episode  12 - Steps :  160 | Total Rewards : -110.45 | Loss : 1.69e+00 | Epsilon :  0.69 | Agent Step : 1384\n",
      "Episode  13 - Steps :   94 | Total Rewards :  -15.52 | Loss : 1.68e+00 | Epsilon :  0.68 | Agent Step : 1478\n",
      "Episode  14 - Steps :  147 | Total Rewards : -125.51 | Loss : 1.62e+00 | Epsilon :  0.66 | Agent Step : 1625\n",
      "Episode  15 - Steps :   92 | Total Rewards :  -69.87 | Loss : 1.52e+00 | Epsilon :  0.65 | Agent Step : 1717\n",
      "Episode  16 - Steps :  101 | Total Rewards :  -88.71 | Loss : 1.78e+00 | Epsilon :  0.64 | Agent Step : 1818\n",
      "Episode  17 - Steps :  114 | Total Rewards :  -67.98 | Loss : 1.60e+00 | Epsilon :  0.63 | Agent Step : 1932\n",
      "Episode  18 - Steps :  160 | Total Rewards :  -54.27 | Loss : 1.51e+00 | Epsilon :  0.61 | Agent Step : 2092\n",
      "Episode  19 - Steps : 1000 | Total Rewards :  103.23 | Loss : 1.67e+00 | Epsilon :  0.51 | Agent Step : 3092\n",
      "Episode  20 - Steps :  146 | Total Rewards :  -56.32 | Loss : 1.46e+00 | Epsilon :  0.49 | Agent Step : 3238\n",
      "Episode  21 - Steps :  386 | Total Rewards :  -17.44 | Loss : 1.50e+00 | Epsilon :  0.46 | Agent Step : 3624\n",
      "Episode  22 - Steps :  136 | Total Rewards :   17.28 | Loss : 1.44e+00 | Epsilon :  0.45 | Agent Step : 3760\n",
      "Episode  23 - Steps :  203 | Total Rewards :  -50.04 | Loss : 1.43e+00 | Epsilon :  0.43 | Agent Step : 3963\n",
      "Episode  24 - Steps :   71 | Total Rewards :  -68.34 | Loss : 1.44e+00 | Epsilon :  0.43 | Agent Step : 4034\n",
      "Episode  25 - Steps :  280 | Total Rewards :  -30.39 | Loss : 1.50e+00 | Epsilon :  0.41 | Agent Step : 4314\n",
      "Episode  26 - Steps :  528 | Total Rewards :  -33.43 | Loss : 1.41e+00 | Epsilon :  0.37 | Agent Step : 4842\n",
      "Episode  27 - Steps : 1000 | Total Rewards :   68.85 | Loss : 1.40e+00 | Epsilon :  0.31 | Agent Step : 5842\n",
      "Episode  28 - Steps : 1000 | Total Rewards :   22.97 | Loss : 1.33e+00 | Epsilon :  0.27 | Agent Step : 6842\n",
      "Episode  29 - Steps :  405 | Total Rewards : -208.51 | Loss : 1.22e+00 | Epsilon :  0.25 | Agent Step : 7247\n",
      "Episode  30 - Steps :  731 | Total Rewards : -219.97 | Loss : 1.16e+00 | Epsilon :  0.22 | Agent Step : 7978\n",
      "Episode  31 - Steps :  705 | Total Rewards : -186.81 | Loss : 1.07e+00 | Epsilon :  0.20 | Agent Step : 8683\n",
      "Episode  32 - Steps :  664 | Total Rewards : -195.90 | Loss : 9.83e-01 | Epsilon :  0.18 | Agent Step : 9347\n",
      "Episode  33 - Steps :  153 | Total Rewards :   -7.65 | Loss : 1.01e+00 | Epsilon :  0.18 | Agent Step : 9500\n",
      "Episode  34 - Steps : 1000 | Total Rewards :  -62.08 | Loss : 9.60e-01 | Epsilon :  0.15 | Agent Step : 10500\n",
      "Episode  35 - Steps : 1000 | Total Rewards :  -42.49 | Loss : 9.10e-01 | Epsilon :  0.14 | Agent Step : 11500\n",
      "Episode  36 - Steps : 1000 | Total Rewards :  -19.93 | Loss : 8.48e-01 | Epsilon :  0.12 | Agent Step : 12500\n",
      "Episode  37 - Steps : 1000 | Total Rewards :  -17.21 | Loss : 7.97e-01 | Epsilon :  0.11 | Agent Step : 13500\n",
      "Episode  38 - Steps :  959 | Total Rewards :  224.51 | Loss : 8.12e-01 | Epsilon :  0.10 | Agent Step : 14459\n",
      "Episode  39 - Steps : 1000 | Total Rewards :  -17.24 | Loss : 7.69e-01 | Epsilon :  0.09 | Agent Step : 15459\n",
      "Episode  40 - Steps :  127 | Total Rewards :  -27.14 | Loss : 6.92e-01 | Epsilon :  0.09 | Agent Step : 15586\n",
      "Episode  41 - Steps : 1000 | Total Rewards :   -2.40 | Loss : 7.79e-01 | Epsilon :  0.08 | Agent Step : 16586\n",
      "Episode  42 - Steps : 1000 | Total Rewards :    2.22 | Loss : 7.52e-01 | Epsilon :  0.08 | Agent Step : 17586\n",
      "Episode  43 - Steps :  192 | Total Rewards :    5.77 | Loss : 7.35e-01 | Epsilon :  0.07 | Agent Step : 17778\n",
      "Episode  44 - Steps : 1000 | Total Rewards :  -20.63 | Loss : 7.24e-01 | Epsilon :  0.07 | Agent Step : 18778\n",
      "Episode  45 - Steps : 1000 | Total Rewards :  -39.82 | Loss : 6.92e-01 | Epsilon :  0.07 | Agent Step : 19778\n",
      "Episode  46 - Steps :  276 | Total Rewards :  270.97 | Loss : 7.23e-01 | Epsilon :  0.07 | Agent Step : 20054\n",
      "Episode  47 - Steps : 1000 | Total Rewards :   13.74 | Loss : 7.07e-01 | Epsilon :  0.06 | Agent Step : 21054\n",
      "Episode  48 - Steps : 1000 | Total Rewards :  -34.02 | Loss : 6.69e-01 | Epsilon :  0.06 | Agent Step : 22054\n",
      "Episode  49 - Steps : 1000 | Total Rewards :    1.40 | Loss : 6.48e-01 | Epsilon :  0.06 | Agent Step : 23054\n",
      "Episode  50 - Steps : 1000 | Total Rewards :    6.39 | Loss : 6.35e-01 | Epsilon :  0.06 | Agent Step : 24054\n",
      "Episode  51 - Steps : 1000 | Total Rewards :  -54.03 | Loss : 6.16e-01 | Epsilon :  0.06 | Agent Step : 25054\n",
      "Episode  52 - Steps : 1000 | Total Rewards :  -20.42 | Loss : 5.95e-01 | Epsilon :  0.05 | Agent Step : 26054\n",
      "Episode  53 - Steps : 1000 | Total Rewards :  -30.66 | Loss : 5.69e-01 | Epsilon :  0.05 | Agent Step : 27054\n",
      "Episode  54 - Steps : 1000 | Total Rewards :  -10.98 | Loss : 5.59e-01 | Epsilon :  0.05 | Agent Step : 28054\n",
      "Episode  55 - Steps :  326 | Total Rewards :  276.65 | Loss : 5.72e-01 | Epsilon :  0.05 | Agent Step : 28380\n",
      "Episode  56 - Steps : 1000 | Total Rewards :  -40.69 | Loss : 5.43e-01 | Epsilon :  0.05 | Agent Step : 29380\n",
      "Episode  57 - Steps : 1000 | Total Rewards :  -61.83 | Loss : 5.25e-01 | Epsilon :  0.05 | Agent Step : 30380\n",
      "Episode  58 - Steps :  821 | Total Rewards : -504.52 | Loss : 5.03e-01 | Epsilon :  0.05 | Agent Step : 31201\n",
      "Episode  59 - Steps : 1000 | Total Rewards :  -54.11 | Loss : 5.14e-01 | Epsilon :  0.05 | Agent Step : 32201\n",
      "Episode  60 - Steps :  460 | Total Rewards :  206.30 | Loss : 5.38e-01 | Epsilon :  0.05 | Agent Step : 32661\n",
      "Episode  61 - Steps : 1000 | Total Rewards :  -37.85 | Loss : 5.19e-01 | Epsilon :  0.05 | Agent Step : 33661\n",
      "Episode  62 - Steps : 1000 | Total Rewards :  -64.08 | Loss : 5.21e-01 | Epsilon :  0.05 | Agent Step : 34661\n",
      "Episode  63 - Steps : 1000 | Total Rewards :  -27.82 | Loss : 5.02e-01 | Epsilon :  0.05 | Agent Step : 35661\n",
      "Episode  64 - Steps : 1000 | Total Rewards :  -41.87 | Loss : 5.11e-01 | Epsilon :  0.05 | Agent Step : 36661\n",
      "Episode  65 - Steps : 1000 | Total Rewards :  -13.46 | Loss : 5.21e-01 | Epsilon :  0.05 | Agent Step : 37661\n",
      "Episode  66 - Steps : 1000 | Total Rewards :  -59.80 | Loss : 5.04e-01 | Epsilon :  0.05 | Agent Step : 38661\n",
      "Episode  67 - Steps : 1000 | Total Rewards :    9.43 | Loss : 4.73e-01 | Epsilon :  0.05 | Agent Step : 39661\n",
      "Episode  68 - Steps : 1000 | Total Rewards :   31.22 | Loss : 4.81e-01 | Epsilon :  0.05 | Agent Step : 40661\n",
      "Episode  69 - Steps : 1000 | Total Rewards :  -63.96 | Loss : 4.73e-01 | Epsilon :  0.05 | Agent Step : 41661\n",
      "Episode  70 - Steps : 1000 | Total Rewards :   -6.11 | Loss : 4.94e-01 | Epsilon :  0.05 | Agent Step : 42661\n",
      "Episode  71 - Steps : 1000 | Total Rewards :   -7.43 | Loss : 4.65e-01 | Epsilon :  0.05 | Agent Step : 43661\n",
      "Episode  72 - Steps : 1000 | Total Rewards :  -19.79 | Loss : 4.55e-01 | Epsilon :  0.05 | Agent Step : 44661\n",
      "Episode  73 - Steps : 1000 | Total Rewards :   25.46 | Loss : 4.64e-01 | Epsilon :  0.05 | Agent Step : 45661\n",
      "Episode  74 - Steps : 1000 | Total Rewards :  -10.41 | Loss : 4.47e-01 | Epsilon :  0.05 | Agent Step : 46661\n",
      "Episode  75 - Steps : 1000 | Total Rewards :  -31.00 | Loss : 4.22e-01 | Epsilon :  0.05 | Agent Step : 47661\n",
      "Episode  76 - Steps : 1000 | Total Rewards :   22.23 | Loss : 4.18e-01 | Epsilon :  0.05 | Agent Step : 48661\n",
      "Episode  77 - Steps :  628 | Total Rewards :  196.10 | Loss : 4.03e-01 | Epsilon :  0.05 | Agent Step : 49289\n",
      "Episode  78 - Steps :  734 | Total Rewards :  185.09 | Loss : 4.33e-01 | Epsilon :  0.05 | Agent Step : 50023\n",
      "Episode  79 - Steps : 1000 | Total Rewards :   22.87 | Loss : 4.36e-01 | Epsilon :  0.05 | Agent Step : 51023\n",
      "Episode  80 - Steps : 1000 | Total Rewards :    1.77 | Loss : 4.18e-01 | Epsilon :  0.05 | Agent Step : 52023\n",
      "Episode  81 - Steps :  439 | Total Rewards :  250.56 | Loss : 4.19e-01 | Epsilon :  0.05 | Agent Step : 52462\n",
      "Episode  82 - Steps :  737 | Total Rewards :  164.33 | Loss : 4.01e-01 | Epsilon :  0.05 | Agent Step : 53199\n",
      "Episode  83 - Steps :  418 | Total Rewards :  236.29 | Loss : 4.30e-01 | Epsilon :  0.05 | Agent Step : 53617\n",
      "Episode  84 - Steps :  602 | Total Rewards :  201.58 | Loss : 3.89e-01 | Epsilon :  0.05 | Agent Step : 54219\n",
      "Episode  85 - Steps :  364 | Total Rewards :  286.64 | Loss : 3.95e-01 | Epsilon :  0.05 | Agent Step : 54583\n",
      "Episode  86 - Steps :  319 | Total Rewards :  224.70 | Loss : 3.80e-01 | Epsilon :  0.05 | Agent Step : 54902\n",
      "Episode  87 - Steps :  633 | Total Rewards :  193.71 | Loss : 3.95e-01 | Epsilon :  0.05 | Agent Step : 55535\n",
      "Episode  88 - Steps :  416 | Total Rewards :  266.10 | Loss : 3.82e-01 | Epsilon :  0.05 | Agent Step : 55951\n",
      "Episode  89 - Steps :  817 | Total Rewards :  207.41 | Loss : 3.93e-01 | Epsilon :  0.05 | Agent Step : 56768\n",
      "Episode  90 - Steps :  295 | Total Rewards :  276.40 | Loss : 3.81e-01 | Epsilon :  0.05 | Agent Step : 57063\n",
      "Episode  91 - Steps :  465 | Total Rewards :  267.90 | Loss : 3.65e-01 | Epsilon :  0.05 | Agent Step : 57528\n",
      "Episode  92 - Steps :  467 | Total Rewards :  257.69 | Loss : 4.14e-01 | Epsilon :  0.05 | Agent Step : 57995\n",
      "Episode  93 - Steps :  446 | Total Rewards :  266.94 | Loss : 3.95e-01 | Epsilon :  0.05 | Agent Step : 58441\n",
      "Episode  94 - Steps :  386 | Total Rewards :  283.05 | Loss : 4.07e-01 | Epsilon :  0.05 | Agent Step : 58827\n",
      "Episode  95 - Steps :  382 | Total Rewards :  240.32 | Loss : 3.85e-01 | Epsilon :  0.05 | Agent Step : 59209\n",
      "Episode  96 - Steps :  363 | Total Rewards :  261.68 | Loss : 4.24e-01 | Epsilon :  0.05 | Agent Step : 59572\n",
      "Episode  97 - Steps :  367 | Total Rewards :  223.24 | Loss : 4.13e-01 | Epsilon :  0.05 | Agent Step : 59939\n",
      "Episode  98 - Steps :  620 | Total Rewards : -208.51 | Loss : 4.15e-01 | Epsilon :  0.05 | Agent Step : 60559\n",
      "Episode  99 - Steps :  538 | Total Rewards :  239.82 | Loss : 4.23e-01 | Epsilon :  0.05 | Agent Step : 61097\n",
      "Episode 100 - Steps :  303 | Total Rewards :  283.19 | Loss : 4.26e-01 | Epsilon :  0.05 | Agent Step : 61400\n",
      "Episode 101 - Steps :  455 | Total Rewards :  278.71 | Loss : 4.14e-01 | Epsilon :  0.05 | Agent Step : 61855\n",
      "Episode 102 - Steps :  319 | Total Rewards :  214.97 | Loss : 4.21e-01 | Epsilon :  0.05 | Agent Step : 62174\n",
      "Episode 103 - Steps :  275 | Total Rewards :  240.57 | Loss : 4.39e-01 | Epsilon :  0.05 | Agent Step : 62449\n",
      "Episode 104 - Steps :  265 | Total Rewards :  266.38 | Loss : 3.96e-01 | Epsilon :  0.05 | Agent Step : 62714\n",
      "Episode 105 - Steps :  275 | Total Rewards :  238.66 | Loss : 4.11e-01 | Epsilon :  0.05 | Agent Step : 62989\n",
      "Episode 106 - Steps :  216 | Total Rewards :  270.22 | Loss : 4.46e-01 | Epsilon :  0.05 | Agent Step : 63205\n",
      "Episode 107 - Steps :  683 | Total Rewards :  228.58 | Loss : 4.11e-01 | Epsilon :  0.05 | Agent Step : 63888\n",
      "Episode 108 - Steps :  385 | Total Rewards : -158.39 | Loss : 4.07e-01 | Epsilon :  0.05 | Agent Step : 64273\n",
      "Episode 109 - Steps :  455 | Total Rewards : -160.18 | Loss : 4.38e-01 | Epsilon :  0.05 | Agent Step : 64728\n",
      "Episode 110 - Steps :  750 | Total Rewards :  228.61 | Loss : 4.36e-01 | Epsilon :  0.05 | Agent Step : 65478\n",
      "Episode 111 - Steps :  598 | Total Rewards : -177.23 | Loss : 4.27e-01 | Epsilon :  0.05 | Agent Step : 66076\n",
      "Episode 112 - Steps :  695 | Total Rewards : -192.68 | Loss : 4.38e-01 | Epsilon :  0.05 | Agent Step : 66771\n",
      "Episode 113 - Steps :  447 | Total Rewards :  268.96 | Loss : 4.51e-01 | Epsilon :  0.05 | Agent Step : 67218\n",
      "Episode 114 - Steps : 1000 | Total Rewards :  -95.68 | Loss : 4.29e-01 | Epsilon :  0.05 | Agent Step : 68218\n",
      "Episode 115 - Steps :  430 | Total Rewards :  279.24 | Loss : 4.36e-01 | Epsilon :  0.05 | Agent Step : 68648\n",
      "Episode 116 - Steps : 1000 | Total Rewards :  -95.49 | Loss : 4.23e-01 | Epsilon :  0.05 | Agent Step : 69648\n",
      "Episode 117 - Steps :  269 | Total Rewards :  241.34 | Loss : 4.41e-01 | Epsilon :  0.05 | Agent Step : 69917\n",
      "Episode 118 - Steps :  318 | Total Rewards :  284.56 | Loss : 4.61e-01 | Epsilon :  0.05 | Agent Step : 70235\n",
      "Episode 119 - Steps : 1000 | Total Rewards :  -42.47 | Loss : 4.28e-01 | Epsilon :  0.05 | Agent Step : 71235\n",
      "Episode 120 - Steps :  265 | Total Rewards :  279.53 | Loss : 4.26e-01 | Epsilon :  0.05 | Agent Step : 71500\n",
      "Episode 121 - Steps :  243 | Total Rewards :  242.63 | Loss : 4.39e-01 | Epsilon :  0.05 | Agent Step : 71743\n",
      "Episode 122 - Steps :  402 | Total Rewards :  264.02 | Loss : 4.00e-01 | Epsilon :  0.05 | Agent Step : 72145\n",
      "Episode 123 - Steps :  342 | Total Rewards :  280.00 | Loss : 4.43e-01 | Epsilon :  0.05 | Agent Step : 72487\n",
      "Episode 124 - Steps :  246 | Total Rewards :  284.12 | Loss : 4.35e-01 | Epsilon :  0.05 | Agent Step : 72733\n",
      "Episode 125 - Steps :  318 | Total Rewards :  260.97 | Loss : 4.51e-01 | Epsilon :  0.05 | Agent Step : 73051\n",
      "Episode 126 - Steps :  301 | Total Rewards :  234.08 | Loss : 4.53e-01 | Epsilon :  0.05 | Agent Step : 73352\n",
      "Episode 127 - Steps :  464 | Total Rewards :  234.04 | Loss : 4.52e-01 | Epsilon :  0.05 | Agent Step : 73816\n",
      "Episode 128 - Steps :  235 | Total Rewards :  240.42 | Loss : 4.55e-01 | Epsilon :  0.05 | Agent Step : 74051\n",
      "Episode 129 - Steps :  385 | Total Rewards :  270.50 | Loss : 4.18e-01 | Epsilon :  0.05 | Agent Step : 74436\n",
      "Episode 130 - Steps :  238 | Total Rewards :  274.13 | Loss : 4.54e-01 | Epsilon :  0.05 | Agent Step : 74674\n",
      "Episode 131 - Steps :  503 | Total Rewards :  273.08 | Loss : 4.53e-01 | Epsilon :  0.05 | Agent Step : 75177\n",
      "Episode 132 - Steps :  312 | Total Rewards :  257.25 | Loss : 4.30e-01 | Epsilon :  0.05 | Agent Step : 75489\n",
      "Episode 133 - Steps :  283 | Total Rewards :  252.32 | Loss : 4.41e-01 | Epsilon :  0.05 | Agent Step : 75772\n",
      "Episode 134 - Steps :  281 | Total Rewards :  241.10 | Loss : 4.37e-01 | Epsilon :  0.05 | Agent Step : 76053\n",
      "Episode 135 - Steps :  406 | Total Rewards :  272.98 | Loss : 4.35e-01 | Epsilon :  0.05 | Agent Step : 76459\n",
      "Episode 136 - Steps :  359 | Total Rewards :  254.32 | Loss : 4.51e-01 | Epsilon :  0.05 | Agent Step : 76818\n",
      "Episode 137 - Steps :  247 | Total Rewards :  271.79 | Loss : 4.30e-01 | Epsilon :  0.05 | Agent Step : 77065\n",
      "Episode 138 - Steps : 1000 | Total Rewards :  135.61 | Loss : 4.48e-01 | Epsilon :  0.05 | Agent Step : 78065\n",
      "Episode 139 - Steps :  336 | Total Rewards :  239.96 | Loss : 4.34e-01 | Epsilon :  0.05 | Agent Step : 78401\n",
      "Episode 140 - Steps :  328 | Total Rewards :  263.97 | Loss : 4.60e-01 | Epsilon :  0.05 | Agent Step : 78729\n",
      "Episode 141 - Steps :  301 | Total Rewards :  274.13 | Loss : 4.10e-01 | Epsilon :  0.05 | Agent Step : 79030\n",
      "Episode 142 - Steps :  421 | Total Rewards :  258.48 | Loss : 4.97e-01 | Epsilon :  0.05 | Agent Step : 79451\n",
      "Episode 143 - Steps :  264 | Total Rewards :  238.59 | Loss : 4.02e-01 | Epsilon :  0.05 | Agent Step : 79715\n",
      "Episode 144 - Steps :  260 | Total Rewards :  243.91 | Loss : 4.51e-01 | Epsilon :  0.05 | Agent Step : 79975\n",
      "Episode 145 - Steps :  411 | Total Rewards :  211.58 | Loss : 4.66e-01 | Epsilon :  0.05 | Agent Step : 80386\n",
      "Episode 146 - Steps :  560 | Total Rewards :  243.83 | Loss : 4.69e-01 | Epsilon :  0.05 | Agent Step : 80946\n",
      "Episode 147 - Steps :  282 | Total Rewards :  293.34 | Loss : 4.30e-01 | Epsilon :  0.05 | Agent Step : 81228\n",
      "Episode 148 - Steps :  286 | Total Rewards :  260.57 | Loss : 4.60e-01 | Epsilon :  0.05 | Agent Step : 81514\n",
      "Episode 149 - Steps :  627 | Total Rewards :  255.10 | Loss : 4.60e-01 | Epsilon :  0.05 | Agent Step : 82141\n",
      "Episode 150 - Steps :  454 | Total Rewards :  259.56 | Loss : 4.44e-01 | Epsilon :  0.05 | Agent Step : 82595\n",
      "Episode 151 - Steps :  230 | Total Rewards :  225.12 | Loss : 4.47e-01 | Epsilon :  0.05 | Agent Step : 82825\n",
      "Episode 152 - Steps :  250 | Total Rewards :  290.14 | Loss : 4.44e-01 | Epsilon :  0.05 | Agent Step : 83075\n",
      "Episode 153 - Steps :  327 | Total Rewards :  288.65 | Loss : 4.40e-01 | Epsilon :  0.05 | Agent Step : 83402\n",
      "Episode 154 - Steps :  253 | Total Rewards :  277.45 | Loss : 4.69e-01 | Epsilon :  0.05 | Agent Step : 83655\n",
      "Episode 155 - Steps :  694 | Total Rewards :  247.93 | Loss : 4.50e-01 | Epsilon :  0.05 | Agent Step : 84349\n",
      "Episode 156 - Steps :  326 | Total Rewards :   -9.12 | Loss : 4.64e-01 | Epsilon :  0.05 | Agent Step : 84675\n",
      "Episode 157 - Steps :  261 | Total Rewards :  289.47 | Loss : 4.14e-01 | Epsilon :  0.05 | Agent Step : 84936\n",
      "Episode 158 - Steps :  301 | Total Rewards :  289.03 | Loss : 4.68e-01 | Epsilon :  0.05 | Agent Step : 85237\n",
      "Episode 159 - Steps :  302 | Total Rewards :  255.86 | Loss : 4.26e-01 | Epsilon :  0.05 | Agent Step : 85539\n",
      "Episode 160 - Steps :  531 | Total Rewards :  227.30 | Loss : 4.05e-01 | Epsilon :  0.05 | Agent Step : 86070\n",
      "Episode 161 - Steps :  258 | Total Rewards :  286.82 | Loss : 4.46e-01 | Epsilon :  0.05 | Agent Step : 86328\n",
      "Episode 162 - Steps :  312 | Total Rewards :  291.96 | Loss : 4.70e-01 | Epsilon :  0.05 | Agent Step : 86640\n",
      "Episode 163 - Steps :  257 | Total Rewards :  292.96 | Loss : 4.47e-01 | Epsilon :  0.05 | Agent Step : 86897\n",
      "Episode 164 - Steps :  219 | Total Rewards :  245.23 | Loss : 4.90e-01 | Epsilon :  0.05 | Agent Step : 87116\n",
      "Episode 165 - Steps :  314 | Total Rewards :  261.92 | Loss : 4.86e-01 | Epsilon :  0.05 | Agent Step : 87430\n",
      "Episode 166 - Steps :  504 | Total Rewards :  237.44 | Loss : 4.77e-01 | Epsilon :  0.05 | Agent Step : 87934\n",
      "Episode 167 - Steps :  263 | Total Rewards :  294.18 | Loss : 4.72e-01 | Epsilon :  0.05 | Agent Step : 88197\n",
      "Episode 168 - Steps :  283 | Total Rewards :  300.80 | Loss : 4.42e-01 | Epsilon :  0.05 | Agent Step : 88480\n",
      "Episode 169 - Steps :  216 | Total Rewards :  260.76 | Loss : 4.48e-01 | Epsilon :  0.05 | Agent Step : 88696\n",
      "Episode 170 - Steps :  478 | Total Rewards :  253.04 | Loss : 4.92e-01 | Epsilon :  0.05 | Agent Step : 89174\n",
      "Episode 171 - Steps :  247 | Total Rewards :  297.46 | Loss : 4.46e-01 | Epsilon :  0.05 | Agent Step : 89421\n",
      "Episode 172 - Steps :  209 | Total Rewards :  278.76 | Loss : 4.91e-01 | Epsilon :  0.05 | Agent Step : 89630\n",
      "Episode 173 - Steps :  249 | Total Rewards :  298.32 | Loss : 4.36e-01 | Epsilon :  0.05 | Agent Step : 89879\n",
      "Episode 174 - Steps :  347 | Total Rewards :  239.53 | Loss : 4.77e-01 | Epsilon :  0.05 | Agent Step : 90226\n",
      "Episode 175 - Steps :  244 | Total Rewards :  278.42 | Loss : 4.76e-01 | Epsilon :  0.05 | Agent Step : 90470\n",
      "Episode 176 - Steps :  229 | Total Rewards :  288.64 | Loss : 4.78e-01 | Epsilon :  0.05 | Agent Step : 90699\n",
      "Episode 177 - Steps :  243 | Total Rewards :  291.60 | Loss : 4.79e-01 | Epsilon :  0.05 | Agent Step : 90942\n",
      "Episode 178 - Steps :  261 | Total Rewards :  294.00 | Loss : 5.19e-01 | Epsilon :  0.05 | Agent Step : 91203\n",
      "Episode 179 - Steps :  294 | Total Rewards :  296.66 | Loss : 4.77e-01 | Epsilon :  0.05 | Agent Step : 91497\n",
      "Episode 180 - Steps :  249 | Total Rewards :   -9.32 | Loss : 4.71e-01 | Epsilon :  0.05 | Agent Step : 91746\n",
      "Episode 181 - Steps :  282 | Total Rewards :  281.59 | Loss : 4.82e-01 | Epsilon :  0.05 | Agent Step : 92028\n",
      "Episode 182 - Steps :  230 | Total Rewards :  316.91 | Loss : 4.77e-01 | Epsilon :  0.05 | Agent Step : 92258\n",
      "Episode 183 - Steps :  263 | Total Rewards :  261.29 | Loss : 4.71e-01 | Epsilon :  0.05 | Agent Step : 92521\n",
      "Episode 184 - Steps :  222 | Total Rewards :  275.72 | Loss : 4.71e-01 | Epsilon :  0.05 | Agent Step : 92743\n",
      "Episode 185 - Steps :  286 | Total Rewards :  305.30 | Loss : 4.71e-01 | Epsilon :  0.05 | Agent Step : 93029\n",
      "Episode 186 - Steps :  347 | Total Rewards :  269.21 | Loss : 4.71e-01 | Epsilon :  0.05 | Agent Step : 93376\n",
      "Episode 187 - Steps :  248 | Total Rewards :  298.06 | Loss : 4.95e-01 | Epsilon :  0.05 | Agent Step : 93624\n",
      "Episode 188 - Steps :  371 | Total Rewards :  252.44 | Loss : 5.02e-01 | Epsilon :  0.05 | Agent Step : 93995\n",
      "Episode 189 - Steps :  247 | Total Rewards :  278.04 | Loss : 5.48e-01 | Epsilon :  0.05 | Agent Step : 94242\n",
      "Episode 190 - Steps :  314 | Total Rewards :  273.85 | Loss : 4.56e-01 | Epsilon :  0.05 | Agent Step : 94556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m state = next_state\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m loss = \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m episode_steps += \u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m : episode_losses.append(loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# In-place gradient clipping\u001b[39;00m\n\u001b[32m     44\u001b[39m torch.nn.utils.clip_grad_value_(policy_net.parameters(), \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cl√©ment\\Desktop\\RainbowTorch\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 300\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    episode_rewards = 0\n",
    "    episode_losses = []\n",
    "    episode_steps = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        episode_rewards += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = optimize_model()\n",
    "\n",
    "        episode_steps += 1\n",
    "        if loss is not None : episode_losses.append(loss)\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # Œ∏‚Ä≤ ‚Üê œÑ Œ∏ + (1 ‚àíœÑ )Œ∏‚Ä≤\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            metric_to_plot.append(episode_rewards)\n",
    "            epsilon = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            episode_loss = np.array(episode_losses).mean()\n",
    "            print(f\"Episode {i_episode:3d} - Steps : {episode_steps:4d} | Total Rewards : {episode_rewards:7.2f} | Loss : {episode_loss:0.2e} | Epsilon : {epsilon : 0.2f} | Agent Step : {steps_done}\")\n",
    "            # print(episode_losses)\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "# print('Complete')\n",
    "# plot_durations(show_result=True)\n",
    "# plt.ioff()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
